{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5792f6f7-9dd6-469e-a74f-85b6232dedca",
   "metadata": {},
   "source": [
    "This notebook is a acustom version of UMAP currently being edited. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc8c72-9317-4ec1-969c-7775b972adc4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91155627-8ba6-4ae9-b495-76fa46802625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Leland McInnes <leland.mcinnes@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "from __future__ import print_function\n",
    "\n",
    "import locale\n",
    "from warnings import warn\n",
    "import time\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import check_random_state, check_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "except ImportError:\n",
    "    # sklearn.externals.joblib is deprecated in 0.21, will be removed in 0.23\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from scipy.sparse import tril as sparse_tril, triu as sparse_triu\n",
    "import scipy.sparse.csgraph\n",
    "import numba\n",
    "\n",
    "import umap.distances as dist\n",
    "\n",
    "import umap.sparse as sparse\n",
    "\n",
    "from umap.utils import (\n",
    "    submatrix,\n",
    "    ts,\n",
    "    csr_unique,\n",
    "    fast_knn_indices,\n",
    ")\n",
    "from umap.spectral import spectral_layout\n",
    "from umap.layouts import (\n",
    "    optimize_layout_euclidean,\n",
    "    optimize_layout_generic,\n",
    "    optimize_layout_inverse,\n",
    ")\n",
    "\n",
    "from pynndescent import NNDescent\n",
    "from pynndescent.distances import named_distances as pynn_named_distances\n",
    "from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b27c71-cd42-411c-9b1c-3be7a3565eda",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49e97e9-5559-4249-b297-0d09e80c10d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "locale.setlocale(locale.LC_NUMERIC, \"C\")\n",
    "\n",
    "INT32_MIN = np.iinfo(np.int32).min + 1\n",
    "INT32_MAX = np.iinfo(np.int32).max - 1\n",
    "\n",
    "SMOOTH_K_TOLERANCE = 1e-5\n",
    "MIN_K_DIST_SCALE = 1e-3\n",
    "NPY_INFINITY = np.inf\n",
    "\n",
    "DISCONNECTION_DISTANCES = {\n",
    "    \"correlation\": 2,\n",
    "    \"cosine\": 2,\n",
    "    \"hellinger\": 1,\n",
    "    \"jaccard\": 1,\n",
    "    \"dice\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88fcfb0-9fd7-47ed-b589-3ff08b3af7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_iter(container):\n",
    "    for i in container:\n",
    "        if isinstance(i, (list, tuple)):\n",
    "            for j in flatten_iter(i):\n",
    "                yield j\n",
    "        else:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46834d0a-6361-472c-9793-aee14c49a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattened(container):\n",
    "    return tuple(flatten_iter(container))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016e9eae-b574-49fc-9260-974bc598699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def breadth_first_search(adjmat, start, min_vertices):\n",
    "    explored = []\n",
    "    queue = [start]\n",
    "    levels = {}\n",
    "    levels[start] = 0\n",
    "    max_level = np.inf\n",
    "    visited = [start]\n",
    "\n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        explored.append(node)\n",
    "        if max_level == np.inf and len(explored) > min_vertices:\n",
    "            max_level = max(levels.values())\n",
    "\n",
    "        if levels[node] + 1 < max_level:\n",
    "            neighbors = adjmat[node].indices\n",
    "            for neighbour in neighbors:\n",
    "                if neighbour not in visited:\n",
    "                    queue.append(neighbour)\n",
    "                    visited.append(neighbour)\n",
    "\n",
    "                    levels[neighbour] = levels[node] + 1\n",
    "\n",
    "    return np.array(explored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "518d465c-2cde-4535-96b5-b6cac10d6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raise_disconnected_warning(\n",
    "    edges_removed,\n",
    "    vertices_disconnected,\n",
    "    disconnection_distance,\n",
    "    total_rows,\n",
    "    threshold=0.1,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"A simple wrapper function to avoid large amounts of code repetition.\"\"\"\n",
    "    if verbose & (vertices_disconnected == 0) & (edges_removed > 0):\n",
    "        print(\n",
    "            f\"Disconnection_distance = {disconnection_distance} has removed {edges_removed} edges.  \"\n",
    "            f\"This is not a problem as no vertices were disconnected.\"\n",
    "        )\n",
    "    elif (vertices_disconnected > 0) & (\n",
    "        vertices_disconnected <= threshold * total_rows\n",
    "    ):\n",
    "        warn(\n",
    "            f\"A few of your vertices were disconnected from the manifold.  This shouldn't cause problems.\\n\"\n",
    "            f\"Disconnection_distance = {disconnection_distance} has removed {edges_removed} edges.\\n\"\n",
    "            f\"It has only fully disconnected {vertices_disconnected} vertices.\\n\"\n",
    "            f\"Use umap.utils.disconnected_vertices() to identify them.\",\n",
    "        )\n",
    "    elif vertices_disconnected > threshold * total_rows:\n",
    "        warn(\n",
    "            f\"A large number of your vertices were disconnected from the manifold.\\n\"\n",
    "            f\"Disconnection_distance = {disconnection_distance} has removed {edges_removed} edges.\\n\"\n",
    "            f\"It has fully disconnected {vertices_disconnected} vertices.\\n\"\n",
    "            f\"You might consider using find_disconnected_points() to find and remove these points from your data.\\n\"\n",
    "            f\"Use umap.utils.disconnected_vertices() to identify them.\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a8db9f-5e83-4c32-b593-dc148b5ca992",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit(\n",
    "    locals={\n",
    "        \"psum\": numba.types.float32,\n",
    "        \"lo\": numba.types.float32,\n",
    "        \"mid\": numba.types.float32,\n",
    "        \"hi\": numba.types.float32,\n",
    "    },\n",
    "    fastmath=True,\n",
    ")  # benchmarking `parallel=True` shows it to *decrease* performance\n",
    "def smooth_knn_dist(distances, k, n_iter=64, local_connectivity=1.0, bandwidth=1.0):\n",
    "    \"\"\"Compute a continuous version of the distance to the kth nearest\n",
    "    neighbor. That is, this is similar to knn-distance but allows continuous\n",
    "    k values rather than requiring an integral k. In essence we are simply\n",
    "    computing the distance such that the cardinality of fuzzy set we generate\n",
    "    is k.\n",
    "    Parameters\n",
    "    ----------\n",
    "    distances: array of shape (n_samples, n_neighbors)\n",
    "        Distances to nearest neighbors for each sample. Each row should be a\n",
    "        sorted list of distances to a given samples nearest neighbors.\n",
    "    k: float\n",
    "        The number of nearest neighbors to approximate for.\n",
    "    n_iter: int (optional, default 64)\n",
    "        We need to binary search for the correct distance value. This is the\n",
    "        max number of iterations to use in such a search.\n",
    "    local_connectivity: int (optional, default 1)\n",
    "        The local connectivity required -- i.e. the number of nearest\n",
    "        neighbors that should be assumed to be connected at a local level.\n",
    "        The higher this value the more connected the manifold becomes\n",
    "        locally. In practice this should be not more than the local intrinsic\n",
    "        dimension of the manifold.\n",
    "    bandwidth: float (optional, default 1)\n",
    "        The target bandwidth of the kernel, larger values will produce\n",
    "        larger return values.\n",
    "    Returns\n",
    "    -------\n",
    "    knn_dist: array of shape (n_samples,)\n",
    "        The distance to kth nearest neighbor, as suitably approximated.\n",
    "    nn_dist: array of shape (n_samples,)\n",
    "        The distance to the 1st nearest neighbor for each point.\n",
    "    \"\"\"\n",
    "    target = np.log2(k) * bandwidth\n",
    "    rho = np.zeros(distances.shape[0], dtype=np.float32)\n",
    "    result = np.zeros(distances.shape[0], dtype=np.float32)\n",
    "\n",
    "    mean_distances = np.mean(distances)\n",
    "\n",
    "    for i in range(distances.shape[0]):\n",
    "        lo = 0.0\n",
    "        hi = NPY_INFINITY\n",
    "        mid = 1.0\n",
    "\n",
    "        # TODO: This is very inefficient, but will do for now. FIXME\n",
    "        ith_distances = distances[i]\n",
    "        non_zero_dists = ith_distances[ith_distances > 0.0]\n",
    "        if non_zero_dists.shape[0] >= local_connectivity:\n",
    "            index = int(np.floor(local_connectivity))\n",
    "            interpolation = local_connectivity - index\n",
    "            if index > 0:\n",
    "                rho[i] = non_zero_dists[index - 1]\n",
    "                if interpolation > SMOOTH_K_TOLERANCE:\n",
    "                    rho[i] += interpolation * (\n",
    "                        non_zero_dists[index] - non_zero_dists[index - 1]\n",
    "                    )\n",
    "            else:\n",
    "                rho[i] = interpolation * non_zero_dists[0]\n",
    "        elif non_zero_dists.shape[0] > 0:\n",
    "            rho[i] = np.max(non_zero_dists)\n",
    "\n",
    "        for n in range(n_iter):\n",
    "\n",
    "            psum = 0.0\n",
    "            for j in range(1, distances.shape[1]):\n",
    "                d = distances[i, j] - rho[i]\n",
    "                if d > 0:\n",
    "                    psum += np.exp(-(d / mid))\n",
    "                else:\n",
    "                    psum += 1.0\n",
    "\n",
    "            if np.fabs(psum - target) < SMOOTH_K_TOLERANCE:\n",
    "                break\n",
    "\n",
    "            if psum > target:\n",
    "                hi = mid\n",
    "                mid = (lo + hi) / 2.0\n",
    "            else:\n",
    "                lo = mid\n",
    "                if hi == NPY_INFINITY:\n",
    "                    mid *= 2\n",
    "                else:\n",
    "                    mid = (lo + hi) / 2.0\n",
    "\n",
    "        result[i] = mid\n",
    "\n",
    "        # TODO: This is very inefficient, but will do for now. FIXME\n",
    "        if rho[i] > 0.0:\n",
    "            mean_ith_distances = np.mean(ith_distances)\n",
    "            if result[i] < MIN_K_DIST_SCALE * mean_ith_distances:\n",
    "                result[i] = MIN_K_DIST_SCALE * mean_ith_distances\n",
    "        else:\n",
    "            if result[i] < MIN_K_DIST_SCALE * mean_distances:\n",
    "                result[i] = MIN_K_DIST_SCALE * mean_distances\n",
    "\n",
    "    return result, rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c014f1-6926-4b0e-baa9-2dbf3c86a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(\n",
    "    X,\n",
    "    n_neighbors,\n",
    "    metric,\n",
    "    metric_kwds,\n",
    "    angular,\n",
    "    random_state,\n",
    "    low_memory=True,\n",
    "    use_pynndescent=True,\n",
    "    n_jobs=-1,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Compute the ``n_neighbors`` nearest points for each data point in ``X``\n",
    "    under ``metric``. This may be exact, but more likely is approximated via\n",
    "    nearest neighbor descent.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array of shape (n_samples, n_features)\n",
    "        The input data to compute the k-neighbor graph of.\n",
    "    n_neighbors: int\n",
    "        The number of nearest neighbors to compute for each sample in ``X``.\n",
    "    metric: string or callable\n",
    "        The metric to use for the computation.\n",
    "    metric_kwds: dict\n",
    "        Any arguments to pass to the metric computation function.\n",
    "    angular: bool\n",
    "        Whether to use angular rp trees in NN approximation.\n",
    "    random_state: np.random state\n",
    "        The random state to use for approximate NN computations.\n",
    "    low_memory: bool (optional, default True)\n",
    "        Whether to pursue lower memory NNdescent.\n",
    "    verbose: bool (optional, default False)\n",
    "        Whether to print status data during the computation.\n",
    "    Returns\n",
    "    -------\n",
    "    knn_indices: array of shape (n_samples, n_neighbors)\n",
    "        The indices on the ``n_neighbors`` closest points in the dataset.\n",
    "    knn_dists: array of shape (n_samples, n_neighbors)\n",
    "        The distances to the ``n_neighbors`` closest points in the dataset.\n",
    "    rp_forest: list of trees\n",
    "        The random projection forest used for searching (if used, None otherwise).\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(ts(), \"Finding Nearest Neighbors\")\n",
    "\n",
    "    if metric == \"precomputed\":\n",
    "        # Note that this does not support sparse distance matrices yet ...\n",
    "        # Compute indices of n nearest neighbors\n",
    "        knn_indices = fast_knn_indices(X, n_neighbors)\n",
    "        # knn_indices = np.argsort(X)[:, :n_neighbors]\n",
    "        # Compute the nearest neighbor distances\n",
    "        #   (equivalent to np.sort(X)[:,:n_neighbors])\n",
    "        knn_dists = X[np.arange(X.shape[0])[:, None], knn_indices].copy()\n",
    "        # Prune any nearest neighbours that are infinite distance apart.\n",
    "        disconnected_index = knn_dists == np.inf\n",
    "        knn_indices[disconnected_index] = -1\n",
    "\n",
    "        knn_search_index = None\n",
    "    else:\n",
    "        # TODO: Hacked values for now\n",
    "        n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0)))\n",
    "        n_iters = max(5, int(round(np.log2(X.shape[0]))))\n",
    "\n",
    "        knn_search_index = NNDescent(\n",
    "            X,\n",
    "            n_neighbors=n_neighbors,\n",
    "            metric=metric,\n",
    "            metric_kwds=metric_kwds,\n",
    "            random_state=random_state,\n",
    "            n_trees=n_trees,\n",
    "            n_iters=n_iters,\n",
    "            max_candidates=60,\n",
    "            low_memory=low_memory,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose,\n",
    "            compressed=False,\n",
    "        )\n",
    "        knn_indices, knn_dists = knn_search_index.neighbor_graph\n",
    "\n",
    "    if verbose:\n",
    "        print(ts(), \"Finished Nearest Neighbor Search\")\n",
    "    return knn_indices, knn_dists, knn_search_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9fca74-3158-4a29-860f-08a8da2c526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit(\n",
    "    locals={\n",
    "        \"knn_dists\": numba.types.float32[:, ::1],\n",
    "        \"sigmas\": numba.types.float32[::1],\n",
    "        \"rhos\": numba.types.float32[::1],\n",
    "        \"val\": numba.types.float32,\n",
    "    },\n",
    "    parallel=True,\n",
    "    fastmath=True,\n",
    ")\n",
    "def compute_membership_strengths(\n",
    "    knn_indices,\n",
    "    knn_dists,\n",
    "    sigmas,\n",
    "    rhos,\n",
    "    return_dists=False,\n",
    "    bipartite=False,\n",
    "):\n",
    "    \"\"\"Construct the membership strength data for the 1-skeleton of each local\n",
    "    fuzzy simplicial set -- this is formed as a sparse matrix where each row is\n",
    "    a local fuzzy simplicial set, with a membership strength for the\n",
    "    1-simplex to each other data point.\n",
    "    Parameters\n",
    "    ----------\n",
    "    knn_indices: array of shape (n_samples, n_neighbors)\n",
    "        The indices on the ``n_neighbors`` closest points in the dataset.\n",
    "    knn_dists: array of shape (n_samples, n_neighbors)\n",
    "        The distances to the ``n_neighbors`` closest points in the dataset.\n",
    "    sigmas: array of shape(n_samples)\n",
    "        The normalization factor derived from the metric tensor approximation.\n",
    "    rhos: array of shape(n_samples)\n",
    "        The local connectivity adjustment.\n",
    "    return_dists: bool (optional, default False)\n",
    "        Whether to return the pairwise distance associated with each edge.\n",
    "    bipartite: bool (optional, default False)\n",
    "        Does the nearest neighbour set represent a bipartite graph? That is, are the\n",
    "        nearest neighbour indices from the same point set as the row indices?\n",
    "    Returns\n",
    "    -------\n",
    "    rows: array of shape (n_samples * n_neighbors)\n",
    "        Row data for the resulting sparse matrix (coo format)\n",
    "    cols: array of shape (n_samples * n_neighbors)\n",
    "        Column data for the resulting sparse matrix (coo format)\n",
    "    vals: array of shape (n_samples * n_neighbors)\n",
    "        Entries for the resulting sparse matrix (coo format)\n",
    "    dists: array of shape (n_samples * n_neighbors)\n",
    "        Distance associated with each entry in the resulting sparse matrix\n",
    "    \"\"\"\n",
    "    n_samples = knn_indices.shape[0]\n",
    "    n_neighbors = knn_indices.shape[1]\n",
    "\n",
    "    rows = np.zeros(knn_indices.size, dtype=np.int32)\n",
    "    cols = np.zeros(knn_indices.size, dtype=np.int32)\n",
    "    vals = np.zeros(knn_indices.size, dtype=np.float32)\n",
    "    if return_dists:\n",
    "        dists = np.zeros(knn_indices.size, dtype=np.float32)\n",
    "    else:\n",
    "        dists = None\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_neighbors):\n",
    "            if knn_indices[i, j] == -1:\n",
    "                continue  # We didn't get the full knn for i\n",
    "            # If applied to an adjacency matrix points shouldn't be similar to themselves.\n",
    "            # If applied to an incidence matrix (or bipartite) then the row and column indices are different.\n",
    "            if (bipartite == False) & (knn_indices[i, j] == i):\n",
    "                val = 0.0\n",
    "            elif knn_dists[i, j] - rhos[i] <= 0.0 or sigmas[i] == 0.0:\n",
    "                val = 1.0\n",
    "            else:\n",
    "                val = np.exp(-((knn_dists[i, j] - rhos[i]) / (sigmas[i])))\n",
    "\n",
    "            rows[i * n_neighbors + j] = i\n",
    "            cols[i * n_neighbors + j] = knn_indices[i, j]\n",
    "            vals[i * n_neighbors + j] = val\n",
    "            if return_dists:\n",
    "                dists[i * n_neighbors + j] = knn_dists[i, j]\n",
    "\n",
    "    return rows, cols, vals, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6853cd0d-9f95-42b5-a1a5-dd618427aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_simplicial_set(\n",
    "    X,\n",
    "    n_neighbors,\n",
    "    random_state,\n",
    "    metric,\n",
    "    metric_kwds={},\n",
    "    knn_indices=None,\n",
    "    knn_dists=None,\n",
    "    angular=False,\n",
    "    set_op_mix_ratio=1.0,\n",
    "    local_connectivity=1.0,\n",
    "    apply_set_operations=True,\n",
    "    verbose=False,\n",
    "    return_dists=None,\n",
    "):\n",
    "    \"\"\"Given a set of data X, a neighborhood size, and a measure of distance\n",
    "    compute the fuzzy simplicial set (here represented as a fuzzy graph in\n",
    "    the form of a sparse matrix) associated to the data. This is done by\n",
    "    locally approximating geodesic distance at each point, creating a fuzzy\n",
    "    simplicial set for each such point, and then combining all the local\n",
    "    fuzzy simplicial sets into a global one via a fuzzy union.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array of shape (n_samples, n_features)\n",
    "        The data to be modelled as a fuzzy simplicial set.\n",
    "    n_neighbors: int\n",
    "        The number of neighbors to use to approximate geodesic distance.\n",
    "        Larger numbers induce more global estimates of the manifold that can\n",
    "        miss finer detail, while smaller values will focus on fine manifold\n",
    "        structure to the detriment of the larger picture.\n",
    "    random_state: numpy RandomState or equivalent\n",
    "        A state capable being used as a numpy random state.\n",
    "    metric: string or function (optional, default 'euclidean')\n",
    "        The metric to use to compute distances in high dimensional space.\n",
    "        If a string is passed it must match a valid predefined metric. If\n",
    "        a general metric is required a function that takes two 1d arrays and\n",
    "        returns a float can be provided. For performance purposes it is\n",
    "        required that this be a numba jit'd function. Valid string metrics\n",
    "        include:\n",
    "        * euclidean (or l2)\n",
    "        * manhattan (or l1)\n",
    "        * cityblock\n",
    "        * braycurtis\n",
    "        * canberra\n",
    "        * chebyshev\n",
    "        * correlation\n",
    "        * cosine\n",
    "        * dice\n",
    "        * hamming\n",
    "        * jaccard\n",
    "        * kulsinski\n",
    "        * ll_dirichlet\n",
    "        * mahalanobis\n",
    "        * matching\n",
    "        * minkowski\n",
    "        * rogerstanimoto\n",
    "        * russellrao\n",
    "        * seuclidean\n",
    "        * sokalmichener\n",
    "        * sokalsneath\n",
    "        * sqeuclidean\n",
    "        * yule\n",
    "        * wminkowski\n",
    "        Metrics that take arguments (such as minkowski, mahalanobis etc.)\n",
    "        can have arguments passed via the metric_kwds dictionary. At this\n",
    "        time care must be taken and dictionary elements must be ordered\n",
    "        appropriately; this will hopefully be fixed in the future.\n",
    "    metric_kwds: dict (optional, default {})\n",
    "        Arguments to pass on to the metric, such as the ``p`` value for\n",
    "        Minkowski distance.\n",
    "    knn_indices: array of shape (n_samples, n_neighbors) (optional)\n",
    "        If the k-nearest neighbors of each point has already been calculated\n",
    "        you can pass them in here to save computation time. This should be\n",
    "        an array with the indices of the k-nearest neighbors as a row for\n",
    "        each data point.\n",
    "    knn_dists: array of shape (n_samples, n_neighbors) (optional)\n",
    "        If the k-nearest neighbors of each point has already been calculated\n",
    "        you can pass them in here to save computation time. This should be\n",
    "        an array with the distances of the k-nearest neighbors as a row for\n",
    "        each data point.\n",
    "    angular: bool (optional, default False)\n",
    "        Whether to use angular/cosine distance for the random projection\n",
    "        forest for seeding NN-descent to determine approximate nearest\n",
    "        neighbors.\n",
    "    set_op_mix_ratio: float (optional, default 1.0)\n",
    "        Interpolate between (fuzzy) union and intersection as the set operation\n",
    "        used to combine local fuzzy simplicial sets to obtain a global fuzzy\n",
    "        simplicial sets. Both fuzzy set operations use the product t-norm.\n",
    "        The value of this parameter should be between 0.0 and 1.0; a value of\n",
    "        1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy\n",
    "        intersection.\n",
    "    local_connectivity: int (optional, default 1)\n",
    "        The local connectivity required -- i.e. the number of nearest\n",
    "        neighbors that should be assumed to be connected at a local level.\n",
    "        The higher this value the more connected the manifold becomes\n",
    "        locally. In practice this should be not more than the local intrinsic\n",
    "        dimension of the manifold.\n",
    "    verbose: bool (optional, default False)\n",
    "        Whether to report information on the current progress of the algorithm.\n",
    "    return_dists: bool or None (optional, default None)\n",
    "        Whether to return the pairwise distance associated with each edge.\n",
    "    Returns\n",
    "    -------\n",
    "    fuzzy_simplicial_set: coo_matrix\n",
    "        A fuzzy simplicial set represented as a sparse matrix. The (i,\n",
    "        j) entry of the matrix represents the membership strength of the\n",
    "        1-simplex between the ith and jth sample points.\n",
    "    \"\"\"\n",
    "    if knn_indices is None or knn_dists is None:\n",
    "        knn_indices, knn_dists, _ = nearest_neighbors(\n",
    "            X,\n",
    "            n_neighbors,\n",
    "            metric,\n",
    "            metric_kwds,\n",
    "            angular,\n",
    "            random_state,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    knn_dists = knn_dists.astype(np.float32)\n",
    "\n",
    "    sigmas, rhos = smooth_knn_dist(\n",
    "        knn_dists,\n",
    "        float(n_neighbors),\n",
    "        local_connectivity=float(local_connectivity),\n",
    "    )\n",
    "\n",
    "    rows, cols, vals, dists = compute_membership_strengths(\n",
    "        knn_indices, knn_dists, sigmas, rhos, return_dists\n",
    "    )\n",
    "\n",
    "    result = scipy.sparse.coo_matrix(\n",
    "        (vals, (rows, cols)), shape=(X.shape[0], X.shape[0])\n",
    "    )\n",
    "    result.eliminate_zeros()\n",
    "\n",
    "    if apply_set_operations:\n",
    "        transpose = result.transpose()\n",
    "\n",
    "        prod_matrix = result.multiply(transpose)\n",
    "\n",
    "        result = (\n",
    "            set_op_mix_ratio * (result + transpose - prod_matrix)\n",
    "            + (1.0 - set_op_mix_ratio) * prod_matrix\n",
    "        )\n",
    "\n",
    "    result.eliminate_zeros()\n",
    "\n",
    "    if return_dists is None:\n",
    "        return result, sigmas, rhos\n",
    "    else:\n",
    "        if return_dists:\n",
    "            dmat = scipy.sparse.coo_matrix(\n",
    "                (dists, (rows, cols)), shape=(X.shape[0], X.shape[0])\n",
    "            )\n",
    "\n",
    "            dists = dmat.maximum(dmat.transpose()).todok()\n",
    "        else:\n",
    "            dists = None\n",
    "\n",
    "        return result, sigmas, rhos, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "205256b1-7738-4b0b-a27f-9236b9fa6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def fast_intersection(rows, cols, values, target, unknown_dist=1.0, far_dist=5.0):\n",
    "    \"\"\"Under the assumption of categorical distance for the intersecting\n",
    "    simplicial set perform a fast intersection.\n",
    "    Parameters\n",
    "    ----------\n",
    "    rows: array\n",
    "        An array of the row of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    cols: array\n",
    "        An array of the column of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    values: array\n",
    "        An array of the value of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    target: array of shape (n_samples)\n",
    "        The categorical labels to use in the intersection.\n",
    "    unknown_dist: float (optional, default 1.0)\n",
    "        The distance an unknown label (-1) is assumed to be from any point.\n",
    "    far_dist float (optional, default 5.0)\n",
    "        The distance between unmatched labels.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for nz in range(rows.shape[0]):\n",
    "        i = rows[nz]\n",
    "        j = cols[nz]\n",
    "        if (target[i] == -1) or (target[j] == -1):\n",
    "            values[nz] *= np.exp(-unknown_dist)\n",
    "        elif target[i] != target[j]:\n",
    "            values[nz] *= np.exp(-far_dist)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62c03681-80f9-43a0-8f1d-f99383c64186",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit()\n",
    "def fast_metric_intersection(\n",
    "    rows, cols, values, discrete_space, metric, metric_args, scale\n",
    "):\n",
    "    \"\"\"Under the assumption of categorical distance for the intersecting\n",
    "    simplicial set perform a fast intersection.\n",
    "    Parameters\n",
    "    ----------\n",
    "    rows: array\n",
    "        An array of the row of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    cols: array\n",
    "        An array of the column of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    values: array of shape\n",
    "        An array of the values of each non-zero in the sparse matrix\n",
    "        representation.\n",
    "    discrete_space: array of shape (n_samples, n_features)\n",
    "        The vectors of categorical labels to use in the intersection.\n",
    "    metric: numba function\n",
    "        The function used to calculate distance over the target array.\n",
    "    scale: float\n",
    "        A scaling to apply to the metric.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for nz in range(rows.shape[0]):\n",
    "        i = rows[nz]\n",
    "        j = cols[nz]\n",
    "        dist = metric(discrete_space[i], discrete_space[j], *metric_args)\n",
    "        values[nz] *= np.exp(-(scale * dist))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09ba9ab-68e3-432a-9e00-a075dae6df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def reprocess_row(probabilities, k=15, n_iters=32):\n",
    "    target = np.log2(k)\n",
    "\n",
    "    lo = 0.0\n",
    "    hi = NPY_INFINITY\n",
    "    mid = 1.0\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        psum = 0.0\n",
    "        for j in range(probabilities.shape[0]):\n",
    "            psum += pow(probabilities[j], mid)\n",
    "\n",
    "        if np.fabs(psum - target) < SMOOTH_K_TOLERANCE:\n",
    "            break\n",
    "\n",
    "        if psum < target:\n",
    "            hi = mid\n",
    "            mid = (lo + hi) / 2.0\n",
    "        else:\n",
    "            lo = mid\n",
    "            if hi == NPY_INFINITY:\n",
    "                mid *= 2\n",
    "            else:\n",
    "                mid = (lo + hi) / 2.0\n",
    "\n",
    "    return np.power(probabilities, mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1fa586f-11cd-4245-b91b-35c57551994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def reset_local_metrics(simplicial_set_indptr, simplicial_set_data):\n",
    "    for i in range(simplicial_set_indptr.shape[0] - 1):\n",
    "        simplicial_set_data[\n",
    "            simplicial_set_indptr[i] : simplicial_set_indptr[i + 1]\n",
    "        ] = reprocess_row(\n",
    "            simplicial_set_data[simplicial_set_indptr[i] : simplicial_set_indptr[i + 1]]\n",
    "        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dd74ed9-fb6f-4572-875c-7b620da8e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_local_connectivity(simplicial_set, reset_local_metric=False):\n",
    "    \"\"\"Reset the local connectivity requirement -- each data sample should\n",
    "    have complete confidence in at least one 1-simplex in the simplicial set.\n",
    "    We can enforce this by locally rescaling confidences, and then remerging the\n",
    "    different local simplicial sets together.\n",
    "    Parameters\n",
    "    ----------\n",
    "    simplicial_set: sparse matrix\n",
    "        The simplicial set for which to recalculate with respect to local\n",
    "        connectivity.\n",
    "    Returns\n",
    "    -------\n",
    "    simplicial_set: sparse_matrix\n",
    "        The recalculated simplicial set, now with the local connectivity\n",
    "        assumption restored.\n",
    "    \"\"\"\n",
    "    simplicial_set = normalize(simplicial_set, norm=\"max\")\n",
    "    if reset_local_metric:\n",
    "        simplicial_set = simplicial_set.tocsr()\n",
    "        reset_local_metrics(simplicial_set.indptr, simplicial_set.data)\n",
    "        simplicial_set = simplicial_set.tocoo()\n",
    "    transpose = simplicial_set.transpose()\n",
    "    prod_matrix = simplicial_set.multiply(transpose)\n",
    "    simplicial_set = simplicial_set + transpose - prod_matrix\n",
    "    simplicial_set.eliminate_zeros()\n",
    "\n",
    "    return simplicial_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8788a709-174a-40f7-9ee9-d1544058b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_metric_simplicial_set_intersection(\n",
    "    simplicial_set,\n",
    "    discrete_space,\n",
    "    unknown_dist=1.0,\n",
    "    far_dist=5.0,\n",
    "    metric=None,\n",
    "    metric_kws={},\n",
    "    metric_scale=1.0,\n",
    "):\n",
    "    \"\"\"Combine a fuzzy simplicial set with another fuzzy simplicial set\n",
    "    generated from discrete metric data using discrete distances. The target\n",
    "    data is assumed to be categorical label data (a vector of labels),\n",
    "    and this will update the fuzzy simplicial set to respect that label data.\n",
    "    TODO: optional category cardinality based weighting of distance\n",
    "    Parameters\n",
    "    ----------\n",
    "    simplicial_set: sparse matrix\n",
    "        The input fuzzy simplicial set.\n",
    "    discrete_space: array of shape (n_samples)\n",
    "        The categorical labels to use in the intersection.\n",
    "    unknown_dist: float (optional, default 1.0)\n",
    "        The distance an unknown label (-1) is assumed to be from any point.\n",
    "    far_dist: float (optional, default 5.0)\n",
    "        The distance between unmatched labels.\n",
    "    metric: str (optional, default None)\n",
    "        If not None, then use this metric to determine the\n",
    "        distance between values.\n",
    "    metric_scale: float (optional, default 1.0)\n",
    "        If using a custom metric scale the distance values by\n",
    "        this value -- this controls the weighting of the\n",
    "        intersection. Larger values weight more toward target.\n",
    "    Returns\n",
    "    -------\n",
    "    simplicial_set: sparse matrix\n",
    "        The resulting intersected fuzzy simplicial set.\n",
    "    \"\"\"\n",
    "    simplicial_set = simplicial_set.tocoo()\n",
    "\n",
    "    if metric is not None:\n",
    "        # We presume target is now a 2d array, with each row being a\n",
    "        # vector of target info\n",
    "        if metric in dist.named_distances:\n",
    "            metric_func = dist.named_distances[metric]\n",
    "        else:\n",
    "            raise ValueError(\"Discrete intersection metric is not recognized\")\n",
    "\n",
    "        fast_metric_intersection(\n",
    "            simplicial_set.row,\n",
    "            simplicial_set.col,\n",
    "            simplicial_set.data,\n",
    "            discrete_space,\n",
    "            metric_func,\n",
    "            tuple(metric_kws.values()),\n",
    "            metric_scale,\n",
    "        )\n",
    "    else:\n",
    "        fast_intersection(\n",
    "            simplicial_set.row,\n",
    "            simplicial_set.col,\n",
    "            simplicial_set.data,\n",
    "            discrete_space,\n",
    "            unknown_dist,\n",
    "            far_dist,\n",
    "        )\n",
    "\n",
    "    simplicial_set.eliminate_zeros()\n",
    "\n",
    "    return reset_local_connectivity(simplicial_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc4dfa80-277e-492e-9d12-1743ac1d8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_simplicial_set_intersection(\n",
    "    simplicial_set1, simplicial_set2, weight=0.5, right_complement=False\n",
    "):\n",
    "\n",
    "    if right_complement:\n",
    "        result = simplicial_set1.tocoo()\n",
    "    else:\n",
    "        result = (simplicial_set1 + simplicial_set2).tocoo()\n",
    "    left = simplicial_set1.tocsr()\n",
    "    right = simplicial_set2.tocsr()\n",
    "\n",
    "    sparse.general_sset_intersection(\n",
    "        left.indptr,\n",
    "        left.indices,\n",
    "        left.data,\n",
    "        right.indptr,\n",
    "        right.indices,\n",
    "        right.data,\n",
    "        result.row,\n",
    "        result.col,\n",
    "        result.data,\n",
    "        mix_weight=weight,\n",
    "        right_complement=right_complement,\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47babf22-bfb1-4916-b628-bbf5d075e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_simplicial_set_union(simplicial_set1, simplicial_set2):\n",
    "    result = (simplicial_set1 + simplicial_set2).tocoo()\n",
    "    left = simplicial_set1.tocsr()\n",
    "    right = simplicial_set2.tocsr()\n",
    "\n",
    "    sparse.general_sset_union(\n",
    "        left.indptr,\n",
    "        left.indices,\n",
    "        left.data,\n",
    "        right.indptr,\n",
    "        right.indices,\n",
    "        right.data,\n",
    "        result.row,\n",
    "        result.col,\n",
    "        result.data,\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ea0a2e7-a14a-4fff-8d35-5b6a794b80a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epochs_per_sample(weights, n_epochs):\n",
    "    \"\"\"Given a set of weights and number of epochs generate the number of\n",
    "    epochs per sample for each weight.\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights: array of shape (n_1_simplices)\n",
    "        The weights of how much we wish to sample each 1-simplex.\n",
    "    n_epochs: int\n",
    "        The total number of epochs we want to train for.\n",
    "    Returns\n",
    "    -------\n",
    "    An array of number of epochs per sample, one for each 1-simplex.\n",
    "    \"\"\"\n",
    "    result = -1.0 * np.ones(weights.shape[0], dtype=np.float64)\n",
    "    n_samples = n_epochs * (weights / weights.max())\n",
    "    result[n_samples > 0] = float(n_epochs) / np.float64(n_samples[n_samples > 0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "684d4215-b810-4ee6-ac90-d8868a8a0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale coords so that the largest coordinate is max_coords, then add normal-distributed\n",
    "# noise with standard deviation noise\n",
    "def noisy_scale_coords(coords, random_state, max_coord=10.0, noise=0.0001):\n",
    "    expansion = max_coord / np.abs(coords).max()\n",
    "    coords = (coords * expansion).astype(np.float32)\n",
    "    return coords + random_state.normal(scale=noise, size=coords.shape).astype(\n",
    "        np.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a041f287-56b5-4bd0-94d0-fe2459708290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplicial_set_embedding(\n",
    "    data,\n",
    "    graph,\n",
    "    n_components,\n",
    "    initial_alpha,\n",
    "    a,\n",
    "    b,\n",
    "    gamma,\n",
    "    negative_sample_rate,\n",
    "    n_epochs,\n",
    "    init,\n",
    "    random_state,\n",
    "    metric,\n",
    "    metric_kwds,\n",
    "    densmap,\n",
    "    densmap_kwds,\n",
    "    output_dens,\n",
    "    output_metric=dist.named_distances_with_gradients[\"euclidean\"],\n",
    "    output_metric_kwds={},\n",
    "    euclidean_output=True,\n",
    "    parallel=False,\n",
    "    verbose=False,\n",
    "    tqdm_kwds=None,\n",
    "):\n",
    "    \"\"\"Perform a fuzzy simplicial set embedding, using a specified\n",
    "    initialisation method and then minimizing the fuzzy set cross entropy\n",
    "    between the 1-skeletons of the high and low dimensional fuzzy simplicial\n",
    "    sets.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: array of shape (n_samples, n_features)\n",
    "        The source data to be embedded by UMAP.\n",
    "    graph: sparse matrix\n",
    "        The 1-skeleton of the high dimensional fuzzy simplicial set as\n",
    "        represented by a graph for which we require a sparse matrix for the\n",
    "        (weighted) adjacency matrix.\n",
    "    n_components: int\n",
    "        The dimensionality of the euclidean space into which to embed the data.\n",
    "    initial_alpha: float\n",
    "        Initial learning rate for the SGD.\n",
    "    a: float\n",
    "        Parameter of differentiable approximation of right adjoint functor\n",
    "    b: float\n",
    "        Parameter of differentiable approximation of right adjoint functor\n",
    "    gamma: float\n",
    "        Weight to apply to negative samples.\n",
    "    negative_sample_rate: int (optional, default 5)\n",
    "        The number of negative samples to select per positive sample\n",
    "        in the optimization process. Increasing this value will result\n",
    "        in greater repulsive force being applied, greater optimization\n",
    "        cost, but slightly more accuracy.\n",
    "    n_epochs: int (optional, default 0), or list of int\n",
    "        The number of training epochs to be used in optimizing the\n",
    "        low dimensional embedding. Larger values result in more accurate\n",
    "        embeddings. If 0 is specified a value will be selected based on\n",
    "        the size of the input dataset (200 for large datasets, 500 for small).\n",
    "        If a list of int is specified, then the intermediate embeddings at the\n",
    "        different epochs specified in that list are returned in\n",
    "        ``aux_data[\"embedding_list\"]``.\n",
    "    init: string\n",
    "        How to initialize the low dimensional embedding. Options are:\n",
    "            * 'spectral': use a spectral embedding of the fuzzy 1-skeleton\n",
    "            * 'random': assign initial embedding positions at random.\n",
    "            * 'pca': use the first n_components from PCA applied to the input data.\n",
    "            * A numpy array of initial embedding positions.\n",
    "    random_state: numpy RandomState or equivalent\n",
    "        A state capable being used as a numpy random state.\n",
    "    metric: string or callable\n",
    "        The metric used to measure distance in high dimensional space; used if\n",
    "        multiple connected components need to be layed out.\n",
    "    metric_kwds: dict\n",
    "        Key word arguments to be passed to the metric function; used if\n",
    "        multiple connected components need to be layed out.\n",
    "    densmap: bool\n",
    "        Whether to use the density-augmented objective function to optimize\n",
    "        the embedding according to the densMAP algorithm.\n",
    "    densmap_kwds: dict\n",
    "        Key word arguments to be used by the densMAP optimization.\n",
    "    output_dens: bool\n",
    "        Whether to output local radii in the original data and the embedding.\n",
    "    output_metric: function\n",
    "        Function returning the distance between two points in embedding space and\n",
    "        the gradient of the distance wrt the first argument.\n",
    "    output_metric_kwds: dict\n",
    "        Key word arguments to be passed to the output_metric function.\n",
    "    euclidean_output: bool\n",
    "        Whether to use the faster code specialised for euclidean output metrics\n",
    "    parallel: bool (optional, default False)\n",
    "        Whether to run the computation using numba parallel.\n",
    "        Running in parallel is non-deterministic, and is not used\n",
    "        if a random seed has been set, to ensure reproducibility.\n",
    "    verbose: bool (optional, default False)\n",
    "        Whether to report information on the current progress of the algorithm.\n",
    "    tqdm_kwds: dict\n",
    "        Key word arguments to be used by the tqdm progress bar.\n",
    "    Returns\n",
    "    -------\n",
    "    embedding: array of shape (n_samples, n_components)\n",
    "        The optimized of ``graph`` into an ``n_components`` dimensional\n",
    "        euclidean space.\n",
    "    aux_data: dict\n",
    "        Auxiliary output returned with the embedding. When densMAP extension\n",
    "        is turned on, this dictionary includes local radii in the original\n",
    "        data (``rad_orig``) and in the embedding (``rad_emb``).\n",
    "    \"\"\"\n",
    "    graph = graph.tocoo()\n",
    "    graph.sum_duplicates()\n",
    "    n_vertices = graph.shape[1]\n",
    "\n",
    "    # For smaller datasets we can use more epochs\n",
    "    if graph.shape[0] <= 10000:\n",
    "        default_epochs = 500\n",
    "    else:\n",
    "        default_epochs = 200\n",
    "\n",
    "    # Use more epochs for densMAP\n",
    "    if densmap:\n",
    "        default_epochs += 200\n",
    "\n",
    "    if n_epochs is None:\n",
    "        n_epochs = default_epochs\n",
    "\n",
    "    # If n_epoch is a list, get the maximum epoch to reach\n",
    "    n_epochs_max = max(n_epochs) if isinstance(n_epochs, list) else n_epochs\n",
    "\n",
    "    if n_epochs_max > 10:\n",
    "        graph.data[graph.data < (graph.data.max() / float(n_epochs_max))] = 0.0\n",
    "    else:\n",
    "        graph.data[graph.data < (graph.data.max() / float(default_epochs))] = 0.0\n",
    "\n",
    "    graph.eliminate_zeros()\n",
    "\n",
    "    if isinstance(init, str) and init == \"random\":\n",
    "        embedding = random_state.uniform(\n",
    "            low=-10.0, high=10.0, size=(graph.shape[0], n_components)\n",
    "        ).astype(np.float32)\n",
    "    elif isinstance(init, str) and init == \"pca\":\n",
    "        if scipy.sparse.issparse(data):\n",
    "            pca = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "        else:\n",
    "            pca = PCA(n_components=n_components, random_state=random_state)\n",
    "        embedding = pca.fit_transform(data).astype(np.float32)\n",
    "        embedding = noisy_scale_coords(\n",
    "            embedding, random_state, max_coord=10, noise=0.0001\n",
    "        )\n",
    "    elif isinstance(init, str) and init == \"spectral\":\n",
    "        embedding = spectral_layout(\n",
    "            data,\n",
    "            graph,\n",
    "            n_components,\n",
    "            random_state,\n",
    "            metric=metric,\n",
    "            metric_kwds=metric_kwds,\n",
    "        )\n",
    "        # We add a little noise to avoid local minima for optimization to come\n",
    "        embedding = noisy_scale_coords(\n",
    "            embedding, random_state, max_coord=10, noise=0.0001\n",
    "        )\n",
    "    else:\n",
    "        init_data = np.array(init)\n",
    "        if len(init_data.shape) == 2:\n",
    "            if np.unique(init_data, axis=0).shape[0] < init_data.shape[0]:\n",
    "                tree = KDTree(init_data)\n",
    "                dist, ind = tree.query(init_data, k=2)\n",
    "                nndist = np.mean(dist[:, 1])\n",
    "                embedding = init_data + random_state.normal(\n",
    "                    scale=0.001 * nndist, size=init_data.shape\n",
    "                ).astype(np.float32)\n",
    "            else:\n",
    "                embedding = init_data\n",
    "\n",
    "    epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs_max)\n",
    "\n",
    "    head = graph.row\n",
    "    tail = graph.col\n",
    "    weight = graph.data\n",
    "\n",
    "    rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n",
    "\n",
    "    aux_data = {}\n",
    "\n",
    "    if densmap or output_dens:\n",
    "        if verbose:\n",
    "            print(ts() + \" Computing original densities\")\n",
    "\n",
    "        dists = densmap_kwds[\"graph_dists\"]\n",
    "\n",
    "        mu_sum = np.zeros(n_vertices, dtype=np.float32)\n",
    "        ro = np.zeros(n_vertices, dtype=np.float32)\n",
    "        for i in range(len(head)):\n",
    "            j = head[i]\n",
    "            k = tail[i]\n",
    "\n",
    "            D = dists[j, k] * dists[j, k]  # match sq-Euclidean used for embedding\n",
    "            mu = graph.data[i]\n",
    "\n",
    "            ro[j] += mu * D\n",
    "            ro[k] += mu * D\n",
    "            mu_sum[j] += mu\n",
    "            mu_sum[k] += mu\n",
    "\n",
    "        epsilon = 1e-8\n",
    "        ro = np.log(epsilon + (ro / mu_sum))\n",
    "\n",
    "        if densmap:\n",
    "            R = (ro - np.mean(ro)) / np.std(ro)\n",
    "            densmap_kwds[\"mu\"] = graph.data\n",
    "            densmap_kwds[\"mu_sum\"] = mu_sum\n",
    "            densmap_kwds[\"R\"] = R\n",
    "\n",
    "        if output_dens:\n",
    "            aux_data[\"rad_orig\"] = ro\n",
    "\n",
    "    embedding = (\n",
    "        10.0\n",
    "        * (embedding - np.min(embedding, 0))\n",
    "        / (np.max(embedding, 0) - np.min(embedding, 0))\n",
    "    ).astype(np.float32, order=\"C\")\n",
    "\n",
    "    if euclidean_output:\n",
    "        embedding = optimize_layout_euclidean(\n",
    "            embedding,\n",
    "            embedding,\n",
    "            head,\n",
    "            tail,\n",
    "            n_epochs,\n",
    "            n_vertices,\n",
    "            epochs_per_sample,\n",
    "            a,\n",
    "            b,\n",
    "            rng_state,\n",
    "            gamma,\n",
    "            initial_alpha,\n",
    "            negative_sample_rate,\n",
    "            parallel=parallel,\n",
    "            verbose=verbose,\n",
    "            densmap=densmap,\n",
    "            densmap_kwds=densmap_kwds,\n",
    "            tqdm_kwds=tqdm_kwds,\n",
    "            move_other=True,\n",
    "        )\n",
    "    else:\n",
    "        embedding = optimize_layout_generic(\n",
    "            embedding,\n",
    "            embedding,\n",
    "            head,\n",
    "            tail,\n",
    "            n_epochs,\n",
    "            n_vertices,\n",
    "            epochs_per_sample,\n",
    "            a,\n",
    "            b,\n",
    "            rng_state,\n",
    "            gamma,\n",
    "            initial_alpha,\n",
    "            negative_sample_rate,\n",
    "            output_metric,\n",
    "            tuple(output_metric_kwds.values()),\n",
    "            verbose=verbose,\n",
    "            tqdm_kwds=tqdm_kwds,\n",
    "            move_other=True,\n",
    "        )\n",
    "\n",
    "    if isinstance(embedding, list):\n",
    "        aux_data[\"embedding_list\"] = embedding\n",
    "        embedding = embedding[-1].copy()\n",
    "\n",
    "    if output_dens:\n",
    "        if verbose:\n",
    "            print(ts() + \" Computing embedding densities\")\n",
    "\n",
    "        # Compute graph in embedding\n",
    "        (knn_indices, knn_dists, rp_forest,) = nearest_neighbors(\n",
    "            embedding,\n",
    "            densmap_kwds[\"n_neighbors\"],\n",
    "            \"euclidean\",\n",
    "            {},\n",
    "            False,\n",
    "            random_state,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        emb_graph, emb_sigmas, emb_rhos, emb_dists = fuzzy_simplicial_set(\n",
    "            embedding,\n",
    "            densmap_kwds[\"n_neighbors\"],\n",
    "            random_state,\n",
    "            \"euclidean\",\n",
    "            {},\n",
    "            knn_indices,\n",
    "            knn_dists,\n",
    "            verbose=verbose,\n",
    "            return_dists=True,\n",
    "        )\n",
    "\n",
    "        emb_graph = emb_graph.tocoo()\n",
    "        emb_graph.sum_duplicates()\n",
    "        emb_graph.eliminate_zeros()\n",
    "\n",
    "        n_vertices = emb_graph.shape[1]\n",
    "\n",
    "        mu_sum = np.zeros(n_vertices, dtype=np.float32)\n",
    "        re = np.zeros(n_vertices, dtype=np.float32)\n",
    "\n",
    "        head = emb_graph.row\n",
    "        tail = emb_graph.col\n",
    "        for i in range(len(head)):\n",
    "            j = head[i]\n",
    "            k = tail[i]\n",
    "\n",
    "            D = emb_dists[j, k]\n",
    "            mu = emb_graph.data[i]\n",
    "\n",
    "            re[j] += mu * D\n",
    "            re[k] += mu * D\n",
    "            mu_sum[j] += mu\n",
    "            mu_sum[k] += mu\n",
    "\n",
    "        epsilon = 1e-8\n",
    "        re = np.log(epsilon + (re / mu_sum))\n",
    "\n",
    "        aux_data[\"rad_emb\"] = re\n",
    "\n",
    "    return embedding, aux_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f58c8a1b-abe1-4842-aabb-81f09f745688",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def init_transform(indices, weights, embedding):\n",
    "    \"\"\"Given indices and weights and an original embeddings\n",
    "    initialize the positions of new points relative to the\n",
    "    indices and weights (of their neighbors in the source data).\n",
    "    Parameters\n",
    "    ----------\n",
    "    indices: array of shape (n_new_samples, n_neighbors)\n",
    "        The indices of the neighbors of each new sample\n",
    "    weights: array of shape (n_new_samples, n_neighbors)\n",
    "        The membership strengths of associated 1-simplices\n",
    "        for each of the new samples.\n",
    "    embedding: array of shape (n_samples, dim)\n",
    "        The original embedding of the source data.\n",
    "    Returns\n",
    "    -------\n",
    "    new_embedding: array of shape (n_new_samples, dim)\n",
    "        An initial embedding of the new sample points.\n",
    "    \"\"\"\n",
    "    result = np.zeros((indices.shape[0], embedding.shape[1]), dtype=np.float32)\n",
    "\n",
    "    for i in range(indices.shape[0]):\n",
    "        for j in range(indices.shape[1]):\n",
    "            for d in range(embedding.shape[1]):\n",
    "                result[i, d] += weights[i, j] * embedding[indices[i, j], d]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70e31b99-694a-40ee-8a63-c9421633aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_graph_transform(graph, embedding):\n",
    "    \"\"\"Given a bipartite graph representing the 1-simplices and strengths between the\n",
    "    new points and the original data set along with an embedding of the original points\n",
    "    initialize the positions of new points relative to the strengths (of their neighbors in the source data).\n",
    "    If a point is in our original data set it embeds at the original points coordinates.\n",
    "    If a point has no neighbours in our original dataset it embeds as the np.nan vector.\n",
    "    Otherwise a point is the weighted average of it's neighbours embedding locations.\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph: csr_matrix (n_new_samples, n_samples)\n",
    "        A matrix indicating the 1-simplices and their associated strengths.  These strengths should\n",
    "        be values between zero and one and not normalized.  One indicating that the new point was identical\n",
    "        to one of our original points.\n",
    "    embedding: array of shape (n_samples, dim)\n",
    "        The original embedding of the source data.\n",
    "    Returns\n",
    "    -------\n",
    "    new_embedding: array of shape (n_new_samples, dim)\n",
    "        An initial embedding of the new sample points.\n",
    "    \"\"\"\n",
    "    result = np.zeros((graph.shape[0], embedding.shape[1]), dtype=np.float32)\n",
    "\n",
    "    for row_index in range(graph.shape[0]):\n",
    "        num_neighbours = len(graph[row_index].indices)\n",
    "        if num_neighbours == 0:\n",
    "            result[row_index] = np.nan\n",
    "            continue\n",
    "        row_sum = np.sum(graph[row_index])\n",
    "        for col_index in graph[row_index].indices:\n",
    "            if graph[row_index, col_index] == 1:\n",
    "                result[row_index, :] = embedding[col_index, :]\n",
    "                break\n",
    "            for d in range(embedding.shape[1]):\n",
    "                result[row_index, d] += (\n",
    "                    graph[row_index, col_index] / row_sum * embedding[col_index, d]\n",
    "                )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2e81d66-52d5-4bdc-af68-dc9390339fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def init_update(current_init, n_original_samples, indices):\n",
    "    for i in range(n_original_samples, indices.shape[0]):\n",
    "        n = 0\n",
    "        for j in range(indices.shape[1]):\n",
    "            for d in range(current_init.shape[1]):\n",
    "                if indices[i, j] < n_original_samples:\n",
    "                    n += 1\n",
    "                    current_init[i, d] += current_init[indices[i, j], d]\n",
    "        for d in range(current_init.shape[1]):\n",
    "            current_init[i, d] /= n\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37ff38be-6f64-4dec-b823-28fd837b1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ab_params(spread, min_dist):\n",
    "    \"\"\"Fit a, b params for the differentiable curve used in lower\n",
    "    dimensional fuzzy simplicial complex construction. We want the\n",
    "    smooth curve (from a pre-defined family with simple gradient) that\n",
    "    best matches an offset exponential decay.\n",
    "    \"\"\"\n",
    "\n",
    "    def curve(x, a, b):\n",
    "        return 1.0 / (1.0 + a * x ** (2 * b))\n",
    "\n",
    "    xv = np.linspace(0, spread * 3, 300)\n",
    "    yv = np.zeros(xv.shape)\n",
    "    yv[xv < min_dist] = 1.0\n",
    "    yv[xv >= min_dist] = np.exp(-(xv[xv >= min_dist] - min_dist) / spread)\n",
    "    params, covar = curve_fit(curve, xv, yv)\n",
    "    return params[0], params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32291124-abb2-410b-919f-d3bc265c8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UMAP(BaseEstimator):\n",
    "    \"\"\"Uniform Manifold Approximation and Projection\n",
    "    Finds a low dimensional embedding of the data that approximates\n",
    "    an underlying manifold.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_neighbors: float (optional, default 15)\n",
    "        The size of local neighborhood (in terms of number of neighboring\n",
    "        sample points) used for manifold approximation. Larger values\n",
    "        result in more global views of the manifold, while smaller\n",
    "        values result in more local data being preserved. In general\n",
    "        values should be in the range 2 to 100.\n",
    "    n_components: int (optional, default 2)\n",
    "        The dimension of the space to embed into. This defaults to 2 to\n",
    "        provide easy visualization, but can reasonably be set to any\n",
    "        integer value in the range 2 to 100.\n",
    "    metric: string or function (optional, default 'euclidean')\n",
    "        The metric to use to compute distances in high dimensional space.\n",
    "        If a string is passed it must match a valid predefined metric. If\n",
    "        a general metric is required a function that takes two 1d arrays and\n",
    "        returns a float can be provided. For performance purposes it is\n",
    "        required that this be a numba jit'd function. Valid string metrics\n",
    "        include:\n",
    "        * euclidean\n",
    "        * manhattan\n",
    "        * chebyshev\n",
    "        * minkowski\n",
    "        * canberra\n",
    "        * braycurtis\n",
    "        * mahalanobis\n",
    "        * wminkowski\n",
    "        * seuclidean\n",
    "        * cosine\n",
    "        * correlation\n",
    "        * haversine\n",
    "        * hamming\n",
    "        * jaccard\n",
    "        * dice\n",
    "        * russelrao\n",
    "        * kulsinski\n",
    "        * ll_dirichlet\n",
    "        * hellinger\n",
    "        * rogerstanimoto\n",
    "        * sokalmichener\n",
    "        * sokalsneath\n",
    "        * yule\n",
    "        Metrics that take arguments (such as minkowski, mahalanobis etc.)\n",
    "        can have arguments passed via the metric_kwds dictionary. At this\n",
    "        time care must be taken and dictionary elements must be ordered\n",
    "        appropriately; this will hopefully be fixed in the future.\n",
    "    n_epochs: int (optional, default None)\n",
    "        The number of training epochs to be used in optimizing the\n",
    "        low dimensional embedding. Larger values result in more accurate\n",
    "        embeddings. If None is specified a value will be selected based on\n",
    "        the size of the input dataset (200 for large datasets, 500 for small).\n",
    "    learning_rate: float (optional, default 1.0)\n",
    "        The initial learning rate for the embedding optimization.\n",
    "    init: string (optional, default 'spectral')\n",
    "        How to initialize the low dimensional embedding. Options are:\n",
    "            * 'spectral': use a spectral embedding of the fuzzy 1-skeleton\n",
    "            * 'random': assign initial embedding positions at random.\n",
    "            * 'pca': use the first n_components from PCA applied to the input data.\n",
    "            * A numpy array of initial embedding positions.\n",
    "    min_dist: float (optional, default 0.1)\n",
    "        The effective minimum distance between embedded points. Smaller values\n",
    "        will result in a more clustered/clumped embedding where nearby points\n",
    "        on the manifold are drawn closer together, while larger values will\n",
    "        result on a more even dispersal of points. The value should be set\n",
    "        relative to the ``spread`` value, which determines the scale at which\n",
    "        embedded points will be spread out.\n",
    "    spread: float (optional, default 1.0)\n",
    "        The effective scale of embedded points. In combination with ``min_dist``\n",
    "        this determines how clustered/clumped the embedded points are.\n",
    "    low_memory: bool (optional, default True)\n",
    "        For some datasets the nearest neighbor computation can consume a lot of\n",
    "        memory. If you find that UMAP is failing due to memory constraints\n",
    "        consider setting this option to True. This approach is more\n",
    "        computationally expensive, but avoids excessive memory use.\n",
    "    set_op_mix_ratio: float (optional, default 1.0)\n",
    "        Interpolate between (fuzzy) union and intersection as the set operation\n",
    "        used to combine local fuzzy simplicial sets to obtain a global fuzzy\n",
    "        simplicial sets. Both fuzzy set operations use the product t-norm.\n",
    "        The value of this parameter should be between 0.0 and 1.0; a value of\n",
    "        1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy\n",
    "        intersection.\n",
    "    local_connectivity: int (optional, default 1)\n",
    "        The local connectivity required -- i.e. the number of nearest\n",
    "        neighbors that should be assumed to be connected at a local level.\n",
    "        The higher this value the more connected the manifold becomes\n",
    "        locally. In practice this should be not more than the local intrinsic\n",
    "        dimension of the manifold.\n",
    "    repulsion_strength: float (optional, default 1.0)\n",
    "        Weighting applied to negative samples in low dimensional embedding\n",
    "        optimization. Values higher than one will result in greater weight\n",
    "        being given to negative samples.\n",
    "    negative_sample_rate: int (optional, default 5)\n",
    "        The number of negative samples to select per positive sample\n",
    "        in the optimization process. Increasing this value will result\n",
    "        in greater repulsive force being applied, greater optimization\n",
    "        cost, but slightly more accuracy.\n",
    "    transform_queue_size: float (optional, default 4.0)\n",
    "        For transform operations (embedding new points using a trained model\n",
    "        this will control how aggressively to search for nearest neighbors.\n",
    "        Larger values will result in slower performance but more accurate\n",
    "        nearest neighbor evaluation.\n",
    "    a: float (optional, default None)\n",
    "        More specific parameters controlling the embedding. If None these\n",
    "        values are set automatically as determined by ``min_dist`` and\n",
    "        ``spread``.\n",
    "    b: float (optional, default None)\n",
    "        More specific parameters controlling the embedding. If None these\n",
    "        values are set automatically as determined by ``min_dist`` and\n",
    "        ``spread``.\n",
    "    random_state: int, RandomState instance or None, optional (default: None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    metric_kwds: dict (optional, default None)\n",
    "        Arguments to pass on to the metric, such as the ``p`` value for\n",
    "        Minkowski distance. If None then no arguments are passed on.\n",
    "    angular_rp_forest: bool (optional, default False)\n",
    "        Whether to use an angular random projection forest to initialise\n",
    "        the approximate nearest neighbor search. This can be faster, but is\n",
    "        mostly on useful for metric that use an angular style distance such\n",
    "        as cosine, correlation etc. In the case of those metrics angular forests\n",
    "        will be chosen automatically.\n",
    "    target_n_neighbors: int (optional, default -1)\n",
    "        The number of nearest neighbors to use to construct the target simplcial\n",
    "        set. If set to -1 use the ``n_neighbors`` value.\n",
    "    target_metric: string or callable (optional, default 'categorical')\n",
    "        The metric used to measure distance for a target array is using supervised\n",
    "        dimension reduction. By default this is 'categorical' which will measure\n",
    "        distance in terms of whether categories match or are different. Furthermore,\n",
    "        if semi-supervised is required target values of -1 will be trated as\n",
    "        unlabelled under the 'categorical' metric. If the target array takes\n",
    "        continuous values (e.g. for a regression problem) then metric of 'l1'\n",
    "        or 'l2' is probably more appropriate.\n",
    "    target_metric_kwds: dict (optional, default None)\n",
    "        Keyword argument to pass to the target metric when performing\n",
    "        supervised dimension reduction. If None then no arguments are passed on.\n",
    "    target_weight: float (optional, default 0.5)\n",
    "        weighting factor between data topology and target topology. A value of\n",
    "        0.0 weights predominantly on data, a value of 1.0 places a strong emphasis on\n",
    "        target. The default of 0.5 balances the weighting equally between data and\n",
    "        target.\n",
    "    transform_seed: int (optional, default 42)\n",
    "        Random seed used for the stochastic aspects of the transform operation.\n",
    "        This ensures consistency in transform operations.\n",
    "    verbose: bool (optional, default False)\n",
    "        Controls verbosity of logging.\n",
    "    tqdm_kwds: dict (optional, defaul None)\n",
    "        Key word arguments to be used by the tqdm progress bar.\n",
    "    unique: bool (optional, default False)\n",
    "        Controls if the rows of your data should be uniqued before being\n",
    "        embedded.  If you have more duplicates than you have n_neighbour\n",
    "        you can have the identical data points lying in different regions of\n",
    "        your space.  It also violates the definition of a metric.\n",
    "        For to map from internal structures back to your data use the variable\n",
    "        _unique_inverse_.\n",
    "    densmap: bool (optional, default False)\n",
    "        Specifies whether the density-augmented objective of densMAP\n",
    "        should be used for optimization. Turning on this option generates\n",
    "        an embedding where the local densities are encouraged to be correlated\n",
    "        with those in the original space. Parameters below with the prefix 'dens'\n",
    "        further control the behavior of this extension.\n",
    "    dens_lambda: float (optional, default 2.0)\n",
    "        Controls the regularization weight of the density correlation term\n",
    "        in densMAP. Higher values prioritize density preservation over the\n",
    "        UMAP objective, and vice versa for values closer to zero. Setting this\n",
    "        parameter to zero is equivalent to running the original UMAP algorithm.\n",
    "    dens_frac: float (optional, default 0.3)\n",
    "        Controls the fraction of epochs (between 0 and 1) where the\n",
    "        density-augmented objective is used in densMAP. The first\n",
    "        (1 - dens_frac) fraction of epochs optimize the original UMAP objective\n",
    "        before introducing the density correlation term.\n",
    "    dens_var_shift: float (optional, default 0.1)\n",
    "        A small constant added to the variance of local radii in the\n",
    "        embedding when calculating the density correlation objective to\n",
    "        prevent numerical instability from dividing by a small number\n",
    "    output_dens: float (optional, default False)\n",
    "        Determines whether the local radii of the final embedding (an inverse\n",
    "        measure of local density) are computed and returned in addition to\n",
    "        the embedding. If set to True, local radii of the original data\n",
    "        are also included in the output for comparison; the output is a tuple\n",
    "        (embedding, original local radii, embedding local radii). This option\n",
    "        can also be used when densmap=False to calculate the densities for\n",
    "        UMAP embeddings.\n",
    "    disconnection_distance: float (optional, default np.inf or maximal value for bounded distances)\n",
    "        Disconnect any vertices of distance greater than or equal to disconnection_distance when approximating the\n",
    "        manifold via our k-nn graph. This is particularly useful in the case that you have a bounded metric.  The\n",
    "        UMAP assumption that we have a connected manifold can be problematic when you have points that are maximally\n",
    "        different from all the rest of your data.  The connected manifold assumption will make such points have perfect\n",
    "        similarity to a random set of other points.  Too many such points will artificially connect your space.\n",
    "    precomputed_knn: tuple (optional, default (None,None,None))\n",
    "        If the k-nearest neighbors of each point has already been calculated you\n",
    "        can pass them in here to save computation time. The number of nearest\n",
    "        neighbors in the precomputed_knn must be greater or equal to the\n",
    "        n_neighbors parameter. This should be a tuple containing the output\n",
    "        of the nearest_neighbors() function or attributes from a previously fit\n",
    "        UMAP object; (knn_indices, knn_dists, knn_search_index). If you wish to use\n",
    "        k-nearest neighbors data calculated by another package then provide a tuple of\n",
    "        the form (knn_indices, knn_dists). The contents of the tuple should be two numpy\n",
    "        arrays of shape (N, n_neighbors) where N is the number of items in the\n",
    "        input data. The first array should be the integer indices of the nearest\n",
    "        neighbors, and the second array should be the corresponding distances. The\n",
    "        nearest neighbor of each item should be itself, e.g. the nearest neighbor of\n",
    "        item 0 should be 0, the nearest neighbor of item 1 is 1 and so on. Please note\n",
    "        that you will *not* be able to transform new data in this case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_neighbors=15,\n",
    "        n_components=2,\n",
    "        metric=\"euclidean\",\n",
    "        metric_kwds=None,\n",
    "        output_metric=\"euclidean\",\n",
    "        output_metric_kwds=None,\n",
    "        n_epochs=None,\n",
    "        learning_rate=1.0,\n",
    "        init=\"spectral\",\n",
    "        min_dist=0.1,\n",
    "        spread=1.0,\n",
    "        low_memory=True,\n",
    "        n_jobs=-1,\n",
    "        set_op_mix_ratio=1.0,\n",
    "        local_connectivity=1.0,\n",
    "        repulsion_strength=1.0,\n",
    "        negative_sample_rate=5,\n",
    "        transform_queue_size=4.0,\n",
    "        a=None,\n",
    "        b=None,\n",
    "        random_state=None,\n",
    "        angular_rp_forest=False,\n",
    "        target_n_neighbors=-1,\n",
    "        target_metric=\"categorical\",\n",
    "        target_metric_kwds=None,\n",
    "        target_weight=0.5,\n",
    "        transform_seed=42,\n",
    "        transform_mode=\"embedding\",\n",
    "        force_approximation_algorithm=False,\n",
    "        verbose=False,\n",
    "        tqdm_kwds=None,\n",
    "        unique=False,\n",
    "        densmap=False,\n",
    "        dens_lambda=2.0,\n",
    "        dens_frac=0.3,\n",
    "        dens_var_shift=0.1,\n",
    "        output_dens=False,\n",
    "        disconnection_distance=None,\n",
    "        precomputed_knn=(None, None, None),\n",
    "    ):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.output_metric = output_metric\n",
    "        self.target_metric = target_metric\n",
    "        self.metric_kwds = metric_kwds\n",
    "        self.output_metric_kwds = output_metric_kwds\n",
    "        self.n_epochs = n_epochs\n",
    "        self.init = init\n",
    "        self.n_components = n_components\n",
    "        self.repulsion_strength = repulsion_strength\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.spread = spread\n",
    "        self.min_dist = min_dist\n",
    "        self.low_memory = low_memory\n",
    "        self.set_op_mix_ratio = set_op_mix_ratio\n",
    "        self.local_connectivity = local_connectivity\n",
    "        self.negative_sample_rate = negative_sample_rate\n",
    "        self.random_state = random_state\n",
    "        self.angular_rp_forest = angular_rp_forest\n",
    "        self.transform_queue_size = transform_queue_size\n",
    "        self.target_n_neighbors = target_n_neighbors\n",
    "        self.target_metric = target_metric\n",
    "        self.target_metric_kwds = target_metric_kwds\n",
    "        self.target_weight = target_weight\n",
    "        self.transform_seed = transform_seed\n",
    "        self.transform_mode = transform_mode\n",
    "        self.force_approximation_algorithm = force_approximation_algorithm\n",
    "        self.verbose = verbose\n",
    "        self.tqdm_kwds = tqdm_kwds\n",
    "        self.unique = unique\n",
    "\n",
    "        self.densmap = densmap\n",
    "        self.dens_lambda = dens_lambda\n",
    "        self.dens_frac = dens_frac\n",
    "        self.dens_var_shift = dens_var_shift\n",
    "        self.output_dens = output_dens\n",
    "        self.disconnection_distance = disconnection_distance\n",
    "        self.precomputed_knn = precomputed_knn\n",
    "\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def _validate_parameters(self):\n",
    "        if self.set_op_mix_ratio < 0.0 or self.set_op_mix_ratio > 1.0:\n",
    "            raise ValueError(\"set_op_mix_ratio must be between 0.0 and 1.0\")\n",
    "        if self.repulsion_strength < 0.0:\n",
    "            raise ValueError(\"repulsion_strength cannot be negative\")\n",
    "        if self.min_dist > self.spread:\n",
    "            raise ValueError(\"min_dist must be less than or equal to spread\")\n",
    "        if self.min_dist < 0.0:\n",
    "            raise ValueError(\"min_dist cannot be negative\")\n",
    "        if not isinstance(self.init, str) and not isinstance(self.init, np.ndarray):\n",
    "            raise ValueError(\"init must be a string or ndarray\")\n",
    "        if isinstance(self.init, str) and self.init not in (\n",
    "            \"pca\",\n",
    "            \"spectral\",\n",
    "            \"random\",\n",
    "        ):\n",
    "            raise ValueError('string init values must be \"pca\", \"spectral\" or \"random\"')\n",
    "        if (\n",
    "            isinstance(self.init, np.ndarray)\n",
    "            and self.init.shape[1] != self.n_components\n",
    "        ):\n",
    "            raise ValueError(\"init ndarray must match n_components value\")\n",
    "        if not isinstance(self.metric, str) and not callable(self.metric):\n",
    "            raise ValueError(\"metric must be string or callable\")\n",
    "        if self.negative_sample_rate < 0:\n",
    "            raise ValueError(\"negative sample rate must be positive\")\n",
    "        if self._initial_alpha < 0.0:\n",
    "            raise ValueError(\"learning_rate must be positive\")\n",
    "        if self.n_neighbors < 2:\n",
    "            raise ValueError(\"n_neighbors must be greater than 1\")\n",
    "        if self.target_n_neighbors < 2 and self.target_n_neighbors != -1:\n",
    "            raise ValueError(\"target_n_neighbors must be greater than 1\")\n",
    "        if not isinstance(self.n_components, int):\n",
    "            if isinstance(self.n_components, str):\n",
    "                raise ValueError(\"n_components must be an int\")\n",
    "            if self.n_components % 1 != 0:\n",
    "                raise ValueError(\"n_components must be a whole number\")\n",
    "            try:\n",
    "                # this will convert other types of int (eg. numpy int64)\n",
    "                # to Python int\n",
    "                self.n_components = int(self.n_components)\n",
    "            except ValueError:\n",
    "                raise ValueError(\"n_components must be an int\")\n",
    "        if self.n_components < 1:\n",
    "            raise ValueError(\"n_components must be greater than 0\")\n",
    "        self.n_epochs_list = None\n",
    "        if isinstance(self.n_epochs, list) or isinstance(self.n_epochs, tuple) or \\\n",
    "                isinstance(self.n_epochs, np.ndarray):\n",
    "            if not issubclass(np.array(self.n_epochs).dtype.type, np.integer) or \\\n",
    "                    not np.all(np.array(self.n_epochs) >= 0):\n",
    "                raise ValueError(\"n_epochs must be a nonnegative integer \"\n",
    "                                 \"or a list of nonnegative integers\")\n",
    "            self.n_epochs_list = list(self.n_epochs)\n",
    "        elif self.n_epochs is not None and (\n",
    "                self.n_epochs < 0 or not isinstance(self.n_epochs, int)\n",
    "        ):\n",
    "            raise ValueError(\"n_epochs must be a nonnegative integer \"\n",
    "                             \"or a list of nonnegative integers\")\n",
    "        if self.metric_kwds is None:\n",
    "            self._metric_kwds = {}\n",
    "        else:\n",
    "            self._metric_kwds = self.metric_kwds\n",
    "        if self.output_metric_kwds is None:\n",
    "            self._output_metric_kwds = {}\n",
    "        else:\n",
    "            self._output_metric_kwds = self.output_metric_kwds\n",
    "        if self.target_metric_kwds is None:\n",
    "            self._target_metric_kwds = {}\n",
    "        else:\n",
    "            self._target_metric_kwds = self.target_metric_kwds\n",
    "        # check sparsity of data upfront to set proper _input_distance_func &\n",
    "        # save repeated checks later on\n",
    "        if scipy.sparse.isspmatrix_csr(self._raw_data):\n",
    "            self._sparse_data = True\n",
    "        else:\n",
    "            self._sparse_data = False\n",
    "        # set input distance metric & inverse_transform distance metric\n",
    "        if callable(self.metric):\n",
    "            in_returns_grad = self._check_custom_metric(\n",
    "                self.metric, self._metric_kwds, self._raw_data\n",
    "            )\n",
    "            if in_returns_grad:\n",
    "                _m = self.metric\n",
    "\n",
    "                @numba.njit(fastmath=True)\n",
    "                def _dist_only(x, y, *kwds):\n",
    "                    return _m(x, y, *kwds)[0]\n",
    "\n",
    "                self._input_distance_func = _dist_only\n",
    "                self._inverse_distance_func = self.metric\n",
    "            else:\n",
    "                self._input_distance_func = self.metric\n",
    "                self._inverse_distance_func = None\n",
    "                warn(\n",
    "                    \"custom distance metric does not return gradient; inverse_transform will be unavailable. \"\n",
    "                    \"To enable using inverse_transform method, define a distance function that returns a tuple \"\n",
    "                    \"of (distance [float], gradient [np.array])\"\n",
    "                )\n",
    "        elif self.metric == \"precomputed\":\n",
    "            if self.unique:\n",
    "                raise ValueError(\"unique is poorly defined on a precomputed metric\")\n",
    "            warn(\"using precomputed metric; inverse_transform will be unavailable\")\n",
    "            self._input_distance_func = self.metric\n",
    "            self._inverse_distance_func = None\n",
    "        elif self.metric == \"hellinger\" and self._raw_data.min() < 0:\n",
    "            raise ValueError(\"Metric 'hellinger' does not support negative values\")\n",
    "        elif self.metric in dist.named_distances:\n",
    "            if self._sparse_data:\n",
    "                if self.metric in sparse.sparse_named_distances:\n",
    "                    self._input_distance_func = sparse.sparse_named_distances[\n",
    "                        self.metric\n",
    "                    ]\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Metric {} is not supported for sparse data\".format(self.metric)\n",
    "                    )\n",
    "            else:\n",
    "                self._input_distance_func = dist.named_distances[self.metric]\n",
    "            try:\n",
    "                self._inverse_distance_func = dist.named_distances_with_gradients[\n",
    "                    self.metric\n",
    "                ]\n",
    "            except KeyError:\n",
    "                warn(\n",
    "                    \"gradient function is not yet implemented for {} distance metric; \"\n",
    "                    \"inverse_transform will be unavailable\".format(self.metric)\n",
    "                )\n",
    "                self._inverse_distance_func = None\n",
    "        elif self.metric in pynn_named_distances:\n",
    "            if self._sparse_data:\n",
    "                if self.metric in pynn_sparse_named_distances:\n",
    "                    self._input_distance_func = pynn_sparse_named_distances[self.metric]\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Metric {} is not supported for sparse data\".format(self.metric)\n",
    "                    )\n",
    "            else:\n",
    "                self._input_distance_func = pynn_named_distances[self.metric]\n",
    "\n",
    "            warn(\n",
    "                \"gradient function is not yet implemented for {} distance metric; \"\n",
    "                \"inverse_transform will be unavailable\".format(self.metric)\n",
    "            )\n",
    "            self._inverse_distance_func = None\n",
    "        else:\n",
    "            raise ValueError(\"metric is neither callable nor a recognised string\")\n",
    "        # set output distance metric\n",
    "        if callable(self.output_metric):\n",
    "            out_returns_grad = self._check_custom_metric(\n",
    "                self.output_metric, self._output_metric_kwds\n",
    "            )\n",
    "            if out_returns_grad:\n",
    "                self._output_distance_func = self.output_metric\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"custom output_metric must return a tuple of (distance [float], gradient [np.array])\"\n",
    "                )\n",
    "        elif self.output_metric == \"precomputed\":\n",
    "            raise ValueError(\"output_metric cannnot be 'precomputed'\")\n",
    "        elif self.output_metric in dist.named_distances_with_gradients:\n",
    "            self._output_distance_func = dist.named_distances_with_gradients[\n",
    "                self.output_metric\n",
    "            ]\n",
    "        elif self.output_metric in dist.named_distances:\n",
    "            raise ValueError(\n",
    "                \"gradient function is not yet implemented for {}.\".format(\n",
    "                    self.output_metric\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"output_metric is neither callable nor a recognised string\"\n",
    "            )\n",
    "        # set angularity for NN search based on metric\n",
    "        if self.metric in (\n",
    "            \"cosine\",\n",
    "            \"correlation\",\n",
    "            \"dice\",\n",
    "            \"jaccard\",\n",
    "            \"ll_dirichlet\",\n",
    "            \"hellinger\",\n",
    "        ):\n",
    "            self.angular_rp_forest = True\n",
    "\n",
    "        if self.n_jobs < -1 or self.n_jobs == 0:\n",
    "            raise ValueError(\"n_jobs must be a postive integer, or -1 (for all cores)\")\n",
    "\n",
    "        if self.dens_lambda < 0.0:\n",
    "            raise ValueError(\"dens_lambda cannot be negative\")\n",
    "        if self.dens_frac < 0.0 or self.dens_frac > 1.0:\n",
    "            raise ValueError(\"dens_frac must be between 0.0 and 1.0\")\n",
    "        if self.dens_var_shift < 0.0:\n",
    "            raise ValueError(\"dens_var_shift cannot be negative\")\n",
    "\n",
    "        self._densmap_kwds = {\n",
    "            \"lambda\": self.dens_lambda if self.densmap else 0.0,\n",
    "            \"frac\": self.dens_frac if self.densmap else 0.0,\n",
    "            \"var_shift\": self.dens_var_shift,\n",
    "            \"n_neighbors\": self.n_neighbors,\n",
    "        }\n",
    "\n",
    "        if self.densmap:\n",
    "            if self.output_metric not in (\"euclidean\", \"l2\"):\n",
    "                raise ValueError(\n",
    "                    \"Non-Euclidean output metric not supported for densMAP.\"\n",
    "                )\n",
    "\n",
    "        # This will be used to prune all edges of greater than a fixed value from our knn graph.\n",
    "        # We have preset defaults described in DISCONNECTION_DISTANCES for our bounded measures.\n",
    "        # Otherwise a user can pass in their own value.\n",
    "        if self.disconnection_distance is None:\n",
    "            self._disconnection_distance = DISCONNECTION_DISTANCES.get(\n",
    "                self.metric, np.inf\n",
    "            )\n",
    "        elif isinstance(self.disconnection_distance, int) or isinstance(\n",
    "            self.disconnection_distance, float\n",
    "        ):\n",
    "            self._disconnection_distance = self.disconnection_distance\n",
    "        else:\n",
    "            raise ValueError(\"disconnection_distance must either be None or a numeric.\")\n",
    "\n",
    "        if self.tqdm_kwds is None:\n",
    "            self.tqdm_kwds = {}\n",
    "        else:\n",
    "            if isinstance(self.tqdm_kwds, dict) is False:\n",
    "                raise ValueError(\n",
    "                    \"tqdm_kwds must be a dictionary. Please provide valid tqdm \"\n",
    "                    \"parameters as key value pairs. Valid tqdm parameters can be \"\n",
    "                    \"found here: https://github.com/tqdm/tqdm#parameters\"\n",
    "                )\n",
    "        if \"desc\" not in self.tqdm_kwds:\n",
    "            self.tqdm_kwds[\"desc\"] = \"Epochs completed\"\n",
    "        if \"bar_format\" not in self.tqdm_kwds:\n",
    "            bar_f = \"{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]\"\n",
    "            self.tqdm_kwds[\"bar_format\"] = bar_f\n",
    "\n",
    "        if hasattr(self, \"knn_dists\") and self.knn_dists is not None:\n",
    "            if self.unique:\n",
    "                raise ValueError(\n",
    "                    \"unique is not currently available for \" \"precomputed_knn.\"\n",
    "                )\n",
    "            if not isinstance(self.knn_indices, np.ndarray):\n",
    "                raise ValueError(\"precomputed_knn[0] must be ndarray object.\")\n",
    "            if not isinstance(self.knn_dists, np.ndarray):\n",
    "                raise ValueError(\"precomputed_knn[1] must be ndarray object.\")\n",
    "            if self.knn_dists.shape != self.knn_indices.shape:\n",
    "                raise ValueError(\n",
    "                    \"precomputed_knn[0] and precomputed_knn[1]\"\n",
    "                    \" must be numpy arrays of the same size.\"\n",
    "                )\n",
    "            # #848: warn but proceed if no search index is present\n",
    "            if not isinstance(self.knn_search_index, NNDescent):\n",
    "                warn(\n",
    "                    \"precomputed_knn[2] (knn_search_index) \"\n",
    "                    \"is not an NNDescent object: transforming new data with transform \"\n",
    "                    \"will be unavailable.\"\n",
    "                )\n",
    "            if self.knn_dists.shape[1] < self.n_neighbors:\n",
    "                warn(\n",
    "                    \"precomputed_knn has a lower number of neighbors than \"\n",
    "                    \"n_neighbors parameter. precomputed_knn will be ignored\"\n",
    "                    \" and the k-nn will be computed normally.\"\n",
    "                )\n",
    "                self.knn_indices = None\n",
    "                self.knn_dists = None\n",
    "                self.knn_search_index = None\n",
    "            elif self.knn_dists.shape[0] != self._raw_data.shape[0]:\n",
    "                warn(\n",
    "                    \"precomputed_knn has a different number of samples than the\"\n",
    "                    \" data you are fitting. precomputed_knn will be ignored and\"\n",
    "                    \"the k-nn will be computed normally.\"\n",
    "                )\n",
    "                self.knn_indices = None\n",
    "                self.knn_dists = None\n",
    "                self.knn_search_index = None\n",
    "            elif (\n",
    "                self.knn_dists.shape[0] < 4096\n",
    "                and not self.force_approximation_algorithm\n",
    "            ):\n",
    "                # force_approximation_algorithm is irrelevant for pre-computed knn\n",
    "                # always set it to True which keeps downstream code paths working\n",
    "                self.force_approximation_algorithm = True\n",
    "            elif self.knn_dists.shape[1] > self.n_neighbors:\n",
    "                # if k for precomputed_knn larger than n_neighbors we simply prune it\n",
    "                self.knn_indices = self.knn_indices[:, : self.n_neighbors]\n",
    "                self.knn_dists = self.knn_dists[:, : self.n_neighbors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6ac9fc1-a323-4a6c-936f-1172298b3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_custom_metric(self, metric, kwds, data=None):\n",
    "        # quickly check to determine whether user-defined\n",
    "        # self.metric/self.output_metric returns both distance and gradient\n",
    "        if data is not None:\n",
    "            # if checking the high-dimensional distance metric, test directly on\n",
    "            # input data so we don't risk violating any assumptions potentially\n",
    "            # hard-coded in the metric (e.g., bounded; non-negative)\n",
    "            x, y = data[np.random.randint(0, data.shape[0], 2)]\n",
    "        else:\n",
    "            # if checking the manifold distance metric, simulate some data on a\n",
    "            # reasonable interval with output dimensionality\n",
    "            x, y = np.random.uniform(low=-10, high=10, size=(2, self.n_components))\n",
    "\n",
    "        if scipy.sparse.issparse(data):\n",
    "            metric_out = metric(x.indices, x.data, y.indices, y.data, **kwds)\n",
    "        else:\n",
    "            metric_out = metric(x, y, **kwds)\n",
    "        # True if metric returns iterable of length 2, False otherwise\n",
    "        return hasattr(metric_out, \"__iter__\") and len(metric_out) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3eba98a-0a51-4906-9d88-4c048197eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _populate_combined_params(self, *models):\n",
    "        self.n_neighbors = flattened([m.n_neighbors for m in models])\n",
    "        self.metric = flattened([m.metric for m in models])\n",
    "        self.metric_kwds = flattened([m.metric_kwds for m in models])\n",
    "        self.output_metric = flattened([m.output_metric for m in models])\n",
    "\n",
    "        self.n_epochs = flattened(\n",
    "            [m.n_epochs if m.n_epochs is not None else -1 for m in models]\n",
    "        )\n",
    "        if all([x == -1 for x in self.n_epochs]):\n",
    "            self.n_epochs = None\n",
    "\n",
    "        self.init = flattened([m.init for m in models])\n",
    "        self.n_components = flattened([m.n_components for m in models])\n",
    "        self.repulsion_strength = flattened([m.repulsion_strength for m in models])\n",
    "        self.learning_rate = flattened([m.learning_rate for m in models])\n",
    "\n",
    "        self.spread = flattened([m.spread for m in models])\n",
    "        self.min_dist = flattened([m.min_dist for m in models])\n",
    "        self.low_memory = flattened([m.low_memory for m in models])\n",
    "        self.set_op_mix_ratio = flattened([m.set_op_mix_ratio for m in models])\n",
    "        self.local_connectivity = flattened([m.local_connectivity for m in models])\n",
    "        self.negative_sample_rate = flattened([m.negative_sample_rate for m in models])\n",
    "        self.random_state = flattened([m.random_state for m in models])\n",
    "        self.angular_rp_forest = flattened([m.angular_rp_forest for m in models])\n",
    "        self.transform_queue_size = flattened([m.transform_queue_size for m in models])\n",
    "        self.target_n_neighbors = flattened([m.target_n_neighbors for m in models])\n",
    "        self.target_metric = flattened([m.target_metric for m in models])\n",
    "        self.target_metric_kwds = flattened([m.target_metric_kwds for m in models])\n",
    "        self.target_weight = flattened([m.target_weight for m in models])\n",
    "        self.transform_seed = flattened([m.transform_seed for m in models])\n",
    "        self.force_approximation_algorithm = flattened(\n",
    "            [m.force_approximation_algorithm for m in models]\n",
    "        )\n",
    "        self.verbose = flattened([m.verbose for m in models])\n",
    "        self.unique = flattened([m.unique for m in models])\n",
    "\n",
    "        self.densmap = flattened([m.densmap for m in models])\n",
    "        self.dens_lambda = flattened([m.dens_lambda for m in models])\n",
    "        self.dens_frac = flattened([m.dens_frac for m in models])\n",
    "        self.dens_var_shift = flattened([m.dens_var_shift for m in models])\n",
    "        self.output_dens = flattened([m.output_dens for m in models])\n",
    "\n",
    "        self.a = flattened([m.a for m in models])\n",
    "        self.b = flattened([m.b for m in models])\n",
    "\n",
    "        self._a = flattened([m._a for m in models])\n",
    "        self._b = flattened([m._b for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eec4cf98-fd9d-4b5d-8d13-9e05e9aa478d",
   "metadata": {},
   "outputs": [],
   "source": [
    " def __mul__(self, other):\n",
    "\n",
    "        check_is_fitted(\n",
    "            self, attributes=[\"graph_\"], msg=\"Only fitted UMAP models can be combined\"\n",
    "        )\n",
    "        check_is_fitted(\n",
    "            other, attributes=[\"graph_\"], msg=\"Only fitted UMAP models can be combined\"\n",
    "        )\n",
    "\n",
    "        if self.graph_.shape[0] != other.graph_.shape[0]:\n",
    "            raise ValueError(\"Only models with the equivalent samples can be combined\")\n",
    "\n",
    "        result = UMAP()\n",
    "        result._populate_combined_params(self, other)\n",
    "\n",
    "        result.graph_ = general_simplicial_set_intersection(\n",
    "            self.graph_, other.graph_, 0.5\n",
    "        )\n",
    "        result.graph_ = reset_local_connectivity(result.graph_, True)\n",
    "\n",
    "        if scipy.sparse.csgraph.connected_components(result.graph_)[0] > 1:\n",
    "            warn(\n",
    "                \"Combined graph is not connected but multi-component layout is unsupported. \"\n",
    "                \"Falling back to random initialization.\"\n",
    "            )\n",
    "            init = \"random\"\n",
    "        else:\n",
    "            init = \"spectral\"\n",
    "\n",
    "        result.densmap = np.any(result.densmap)\n",
    "        result.output_dens = np.any(result.output_dens)\n",
    "\n",
    "        result._densmap_kwds = {\n",
    "            \"lambda\": np.max(result.dens_lambda),\n",
    "            \"frac\": np.max(result.dens_frac),\n",
    "            \"var_shift\": np.max(result.dens_var_shift),\n",
    "            \"n_neighbors\": np.max(result.n_neighbors),\n",
    "        }\n",
    "\n",
    "        if result.n_epochs is None:\n",
    "            n_epochs = None\n",
    "        else:\n",
    "            n_epochs = np.max(result.n_epochs)\n",
    "\n",
    "        result.embedding_, aux_data = simplicial_set_embedding(\n",
    "            None,\n",
    "            result.graph_,\n",
    "            np.min(result.n_components),\n",
    "            np.min(result.learning_rate),\n",
    "            np.mean(result._a),\n",
    "            np.mean(result._b),\n",
    "            np.mean(result.repulsion_strength),\n",
    "            np.mean(result.negative_sample_rate),\n",
    "            n_epochs,\n",
    "            init,\n",
    "            check_random_state(42),\n",
    "            \"euclidean\",\n",
    "            {},\n",
    "            result.densmap,\n",
    "            result._densmap_kwds,\n",
    "            result.output_dens,\n",
    "            parallel=False,\n",
    "            verbose=bool(np.max(result.verbose)),\n",
    "            tqdm_kwds=self.tqdm_kwds,\n",
    "        )\n",
    "\n",
    "        if result.output_dens:\n",
    "            result.rad_orig_ = aux_data[\"rad_orig\"]\n",
    "            result.rad_emb_ = aux_data[\"rad_emb\"]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df359c45-0ddf-4a82-9ea1-e0a6473d884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __add__(self, other):\n",
    "\n",
    "        check_is_fitted(\n",
    "            self, attributes=[\"graph_\"], msg=\"Only fitted UMAP models can be combined\"\n",
    "        )\n",
    "        check_is_fitted(\n",
    "            other, attributes=[\"graph_\"], msg=\"Only fitted UMAP models can be combined\"\n",
    "        )\n",
    "\n",
    "        if self.graph_.shape[0] != other.graph_.shape[0]:\n",
    "            raise ValueError(\"Only models with the equivalent samples can be combined\")\n",
    "\n",
    "        result = UMAP()\n",
    "        result._populate_combined_params(self, other)\n",
    "\n",
    "        result.graph_ = general_simplicial_set_union(self.graph_, other.graph_)\n",
    "        result.graph_ = reset_local_connectivity(result.graph_, True)\n",
    "\n",
    "        if scipy.sparse.csgraph.connected_components(result.graph_)[0] > 1:\n",
    "            warn(\n",
    "                \"Combined graph is not connected but mult-component layout is unsupported. \"\n",
    "                \"Falling back to random initialization.\"\n",
    "            )\n",
    "            init = \"random\"\n",
    "        else:\n",
    "            init = \"spectral\"\n",
    "\n",
    "        result.densmap = np.any(result.densmap)\n",
    "        result.output_dens = np.any(result.output_dens)\n",
    "\n",
    "        result._densmap_kwds = {\n",
    "            \"lambda\": np.max(result.dens_lambda),\n",
    "            \"frac\": np.max(result.dens_frac),\n",
    "            \"var_shift\": np.max(result.dens_var_shift),\n",
    "            \"n_neighbors\": np.max(result.n_neighbors),\n",
    "        }\n",
    "\n",
    "        if result.n_epochs is None:\n",
    "            n_epochs = None\n",
    "        else:\n",
    "            n_epochs = np.max(result.n_epochs)\n",
    "\n",
    "        result.embedding_, aux_data = simplicial_set_embedding(\n",
    "            None,\n",
    "            result.graph_,\n",
    "            np.min(result.n_components),\n",
    "            np.min(result.learning_rate),\n",
    "            np.mean(result._a),\n",
    "            np.mean(result._b),\n",
    "            np.mean(result.repulsion_strength),\n",
    "            np.mean(result.negative_sample_rate),\n",
    "            n_epochs,\n",
    "            init,\n",
    "            check_random_state(42),\n",
    "            \"euclidean\",\n",
    "            {},\n",
    "            result.densmap,\n",
    "            result._densmap_kwds,\n",
    "            result.output_dens,\n",
    "            parallel=False,\n",
    "            verbose=bool(np.max(result.verbose)),\n",
    "            tqdm_kwds=self.tqdm_kwds,\n",
    "        )\n",
    "\n",
    "        if result.output_dens:\n",
    "            result.rad_orig_ = aux_data[\"rad_orig\"]\n",
    "            result.rad_emb_ = aux_data[\"rad_emb\"]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5027126d-6ecf-44e2-a291-d0d9a960da82",
   "metadata": {},
   "outputs": [],
   "source": [
    " def __sub__(self, other):\n",
    "\n",
    "        check_is_fitted(\n",
    "            self, attributes=[\"graph_\"], msg=\"Only fitted UMAP models can be combined\"\n",
    "        )\n",
    "        check_is_fitted(\n",
    "            other, attributes=[\"graph_\"], msg=\"Only fitted UMAP models can be combined\"\n",
    "        )\n",
    "\n",
    "        if self.graph_.shape[0] != other.graph_.shape[0]:\n",
    "            raise ValueError(\"Only models with the equivalent samples can be combined\")\n",
    "\n",
    "        result = UMAP()\n",
    "        result._populate_combined_params(self, other)\n",
    "\n",
    "        result.graph_ = general_simplicial_set_intersection(\n",
    "            self.graph_, other.graph_, weight=0.5, right_complement=True\n",
    "        )\n",
    "        result.graph_ = reset_local_connectivity(result.graph_, False)\n",
    "\n",
    "        if scipy.sparse.csgraph.connected_components(result.graph_)[0] > 1:\n",
    "            warn(\n",
    "                \"Combined graph is not connected but mult-component layout is unsupported. \"\n",
    "                \"Falling back to random initialization.\"\n",
    "            )\n",
    "            init = \"random\"\n",
    "        else:\n",
    "            init = \"spectral\"\n",
    "\n",
    "        result.densmap = np.any(result.densmap)\n",
    "        result.output_dens = np.any(result.output_dens)\n",
    "\n",
    "        result._densmap_kwds = {\n",
    "            \"lambda\": np.max(result.dens_lambda),\n",
    "            \"frac\": np.max(result.dens_frac),\n",
    "            \"var_shift\": np.max(result.dens_var_shift),\n",
    "            \"n_neighbors\": np.max(result.n_neighbors),\n",
    "        }\n",
    "\n",
    "        if result.n_epochs is None:\n",
    "            n_epochs = None\n",
    "        else:\n",
    "            n_epochs = np.max(result.n_epochs)\n",
    "\n",
    "        result.embedding_, aux_data = simplicial_set_embedding(\n",
    "            None,\n",
    "            result.graph_,\n",
    "            np.min(result.n_components),\n",
    "            np.min(result.learning_rate),\n",
    "            np.mean(result._a),\n",
    "            np.mean(result._b),\n",
    "            np.mean(result.repulsion_strength),\n",
    "            np.mean(result.negative_sample_rate),\n",
    "            n_epochs,\n",
    "            init,\n",
    "            check_random_state(42),\n",
    "            \"euclidean\",\n",
    "            {},\n",
    "            result.densmap,\n",
    "            result._densmap_kwds,\n",
    "            result.output_dens,\n",
    "            parallel=False,\n",
    "            verbose=bool(np.max(result.verbose)),\n",
    "            tqdm_kwds=self.tqdm_kwds,\n",
    "        )\n",
    "\n",
    "        if result.output_dens:\n",
    "            result.rad_orig_ = aux_data[\"rad_orig\"]\n",
    "            result.rad_emb_ = aux_data[\"rad_emb\"]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fc715ca-7ed1-4cad-98f4-de96df708806",
   "metadata": {},
   "outputs": [],
   "source": [
    " def fit(self, X, y=None):\n",
    "        \"\"\"Fit X into an embedded space.\n",
    "        Optionally use y for supervised dimension reduction.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "            If the metric is 'precomputed' X must be a square distance\n",
    "            matrix. Otherwise it contains a sample per row. If the method\n",
    "            is 'exact', X may be a sparse matrix of type 'csr', 'csc'\n",
    "            or 'coo'.\n",
    "        y : array, shape (n_samples)\n",
    "            A target array for supervised dimension reduction. How this is\n",
    "            handled is determined by parameters UMAP was instantiated with.\n",
    "            The relevant attributes are ``target_metric`` and\n",
    "            ``target_metric_kwds``.\n",
    "        \"\"\"\n",
    "\n",
    "        X = check_array(X, dtype=np.float32, accept_sparse=\"csr\", order=\"C\")\n",
    "        self._raw_data = X\n",
    "\n",
    "        # Handle all the optional arguments, setting default\n",
    "        if self.a is None or self.b is None:\n",
    "            self._a, self._b = find_ab_params(self.spread, self.min_dist)\n",
    "        else:\n",
    "            self._a = self.a\n",
    "            self._b = self.b\n",
    "\n",
    "        if isinstance(self.init, np.ndarray):\n",
    "            init = check_array(self.init, dtype=np.float32, accept_sparse=False)\n",
    "        else:\n",
    "            init = self.init\n",
    "\n",
    "        self._initial_alpha = self.learning_rate\n",
    "\n",
    "        self.knn_indices = self.precomputed_knn[0]\n",
    "        self.knn_dists = self.precomputed_knn[1]\n",
    "        # #848: allow precomputed knn to not have a search index\n",
    "        if len(self.precomputed_knn) == 2:\n",
    "            self.knn_search_index = None\n",
    "        else:\n",
    "            self.knn_search_index = self.precomputed_knn[2]\n",
    "\n",
    "        self._validate_parameters()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(str(self))\n",
    "\n",
    "        self._original_n_threads = numba.get_num_threads()\n",
    "        if self.n_jobs > 0 and self.n_jobs is not None:\n",
    "            numba.set_num_threads(self.n_jobs)\n",
    "\n",
    "        # Check if we should unique the data\n",
    "        # We've already ensured that we aren't in the precomputed case\n",
    "        if self.unique:\n",
    "            # check if the matrix is dense\n",
    "            if self._sparse_data:\n",
    "                # Call a sparse unique function\n",
    "                index, inverse, counts = csr_unique(X)\n",
    "            else:\n",
    "                index, inverse, counts = np.unique(\n",
    "                    X,\n",
    "                    return_index=True,\n",
    "                    return_inverse=True,\n",
    "                    return_counts=True,\n",
    "                    axis=0,\n",
    "                )[1:4]\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    \"Unique=True -> Number of data points reduced from \",\n",
    "                    X.shape[0],\n",
    "                    \" to \",\n",
    "                    X[index].shape[0],\n",
    "                )\n",
    "                most_common = np.argmax(counts)\n",
    "                print(\n",
    "                    \"Most common duplicate is\",\n",
    "                    index[most_common],\n",
    "                    \" with a count of \",\n",
    "                    counts[most_common],\n",
    "                )\n",
    "            # We'll expose an inverse map when unique=True for users to map from our internal structures to their data\n",
    "            self._unique_inverse_ = inverse\n",
    "        # If we aren't asking for unique use the full index.\n",
    "        # This will save special cases later.\n",
    "        else:\n",
    "            index = list(range(X.shape[0]))\n",
    "            inverse = list(range(X.shape[0]))\n",
    "\n",
    "        # Error check n_neighbors based on data size\n",
    "        if X[index].shape[0] <= self.n_neighbors:\n",
    "            if X[index].shape[0] == 1:\n",
    "                self.embedding_ = np.zeros(\n",
    "                    (1, self.n_components)\n",
    "                )  # needed to sklearn comparability\n",
    "                return self\n",
    "\n",
    "            warn(\n",
    "                \"n_neighbors is larger than the dataset size; truncating to \"\n",
    "                \"X.shape[0] - 1\"\n",
    "            )\n",
    "            self._n_neighbors = X[index].shape[0] - 1\n",
    "            if self.densmap:\n",
    "                self._densmap_kwds[\"n_neighbors\"] = self._n_neighbors\n",
    "        else:\n",
    "            self._n_neighbors = self.n_neighbors\n",
    "\n",
    "        # Note: unless it causes issues for setting 'index', could move this to\n",
    "        # initial sparsity check above\n",
    "        if self._sparse_data and not X.has_sorted_indices:\n",
    "            X.sort_indices()\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(ts(), \"Construct fuzzy simplicial set\")\n",
    "\n",
    "        if self.metric == \"precomputed\" and self._sparse_data:\n",
    "            # For sparse precomputed distance matrices, we just argsort the rows to find\n",
    "            # nearest neighbors. To make this easier, we expect matrices that are\n",
    "            # symmetrical (so we can find neighbors by looking at rows in isolation,\n",
    "            # rather than also having to consider that sample's column too).\n",
    "            # print(\"Computing KNNs for sparse precomputed distances...\")\n",
    "            if sparse_tril(X).getnnz() != sparse_triu(X).getnnz():\n",
    "                raise ValueError(\n",
    "                    \"Sparse precomputed distance matrices should be symmetrical!\"\n",
    "                )\n",
    "            if not np.all(X.diagonal() == 0):\n",
    "                raise ValueError(\"Non-zero distances from samples to themselves!\")\n",
    "            if self.knn_dists is None:\n",
    "                self._knn_indices = np.zeros(\n",
    "                    (X.shape[0], self.n_neighbors), dtype=np.int\n",
    "                )\n",
    "                self._knn_dists = np.zeros(self._knn_indices.shape, dtype=np.float)\n",
    "                for row_id in range(X.shape[0]):\n",
    "                    # Find KNNs row-by-row\n",
    "                    row_data = X[row_id].data\n",
    "                    row_indices = X[row_id].indices\n",
    "                    if len(row_data) < self._n_neighbors:\n",
    "                        raise ValueError(\n",
    "                            \"Some rows contain fewer than n_neighbors distances!\"\n",
    "                        )\n",
    "                    row_nn_data_indices = np.argsort(row_data)[: self._n_neighbors]\n",
    "                    self._knn_indices[row_id] = row_indices[row_nn_data_indices]\n",
    "                    self._knn_dists[row_id] = row_data[row_nn_data_indices]\n",
    "            else:\n",
    "                self._knn_indices = self.knn_indices\n",
    "                self._knn_dists = self.knn_dists\n",
    "            # Disconnect any vertices farther apart than _disconnection_distance\n",
    "            disconnected_index = self._knn_dists >= self._disconnection_distance\n",
    "            self._knn_indices[disconnected_index] = -1\n",
    "            self._knn_dists[disconnected_index] = np.inf\n",
    "            edges_removed = disconnected_index.sum()\n",
    "\n",
    "            (\n",
    "                self.graph_,\n",
    "                self._sigmas,\n",
    "                self._rhos,\n",
    "                self.graph_dists_,\n",
    "            ) = fuzzy_simplicial_set(\n",
    "                X[index],\n",
    "                self.n_neighbors,\n",
    "                random_state,\n",
    "                \"precomputed\",\n",
    "                self._metric_kwds,\n",
    "                self._knn_indices,\n",
    "                self._knn_dists,\n",
    "                self.angular_rp_forest,\n",
    "                self.set_op_mix_ratio,\n",
    "                self.local_connectivity,\n",
    "                True,\n",
    "                self.verbose,\n",
    "                self.densmap or self.output_dens,\n",
    "            )\n",
    "            # Report the number of vertices with degree 0 in our our umap.graph_\n",
    "            # This ensures that they were properly disconnected.\n",
    "            vertices_disconnected = np.sum(\n",
    "                np.array(self.graph_.sum(axis=1)).flatten() == 0\n",
    "            )\n",
    "            raise_disconnected_warning(\n",
    "                edges_removed,\n",
    "                vertices_disconnected,\n",
    "                self._disconnection_distance,\n",
    "                self._raw_data.shape[0],\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        # Handle small cases efficiently by computing all distances\n",
    "        elif X[index].shape[0] < 4096 and not self.force_approximation_algorithm:\n",
    "            self._small_data = True\n",
    "            try:\n",
    "                # sklearn pairwise_distances fails for callable metric on sparse data\n",
    "                _m = self.metric if self._sparse_data else self._input_distance_func\n",
    "                dmat = pairwise_distances(X[index], metric=_m, **self._metric_kwds)\n",
    "            except (ValueError, TypeError) as e:\n",
    "                # metric is numba.jit'd or not supported by sklearn,\n",
    "                # fallback to pairwise special\n",
    "\n",
    "                if self._sparse_data:\n",
    "                    # Get a fresh metric since we are casting to dense\n",
    "                    if not callable(self.metric):\n",
    "                        _m = dist.named_distances[self.metric]\n",
    "                        dmat = dist.pairwise_special_metric(\n",
    "                            X[index].toarray(),\n",
    "                            metric=_m,\n",
    "                            kwds=self._metric_kwds,\n",
    "                        )\n",
    "                    else:\n",
    "                        dmat = dist.pairwise_special_metric(\n",
    "                            X[index],\n",
    "                            metric=self._input_distance_func,\n",
    "                            kwds=self._metric_kwds,\n",
    "                        )\n",
    "                else:\n",
    "                    dmat = dist.pairwise_special_metric(\n",
    "                        X[index],\n",
    "                        metric=self._input_distance_func,\n",
    "                        kwds=self._metric_kwds,\n",
    "                    )\n",
    "            # set any values greater than disconnection_distance to be np.inf.\n",
    "            # This will have no effect when _disconnection_distance is not set since it defaults to np.inf.\n",
    "            edges_removed = np.sum(dmat >= self._disconnection_distance)\n",
    "            dmat[dmat >= self._disconnection_distance] = np.inf\n",
    "            (\n",
    "                self.graph_,\n",
    "                self._sigmas,\n",
    "                self._rhos,\n",
    "                self.graph_dists_,\n",
    "            ) = fuzzy_simplicial_set(\n",
    "                dmat,\n",
    "                self._n_neighbors,\n",
    "                random_state,\n",
    "                \"precomputed\",\n",
    "                self._metric_kwds,\n",
    "                None,\n",
    "                None,\n",
    "                self.angular_rp_forest,\n",
    "                self.set_op_mix_ratio,\n",
    "                self.local_connectivity,\n",
    "                True,\n",
    "                self.verbose,\n",
    "                self.densmap or self.output_dens,\n",
    "            )\n",
    "            # Report the number of vertices with degree 0 in our umap.graph_\n",
    "            # This ensures that they were properly disconnected.\n",
    "            vertices_disconnected = np.sum(\n",
    "                np.array(self.graph_.sum(axis=1)).flatten() == 0\n",
    "            )\n",
    "            raise_disconnected_warning(\n",
    "                edges_removed,\n",
    "                vertices_disconnected,\n",
    "                self._disconnection_distance,\n",
    "                self._raw_data.shape[0],\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        else:\n",
    "            # Standard case\n",
    "            self._small_data = False\n",
    "            # Standard case\n",
    "            if self._sparse_data and self.metric in pynn_sparse_named_distances:\n",
    "                nn_metric = self.metric\n",
    "            elif not self._sparse_data and self.metric in pynn_named_distances:\n",
    "                nn_metric = self.metric\n",
    "            else:\n",
    "                nn_metric = self._input_distance_func\n",
    "            if self.knn_dists is None:\n",
    "                (\n",
    "                    self._knn_indices,\n",
    "                    self._knn_dists,\n",
    "                    self._knn_search_index,\n",
    "                ) = nearest_neighbors(\n",
    "                    X[index],\n",
    "                    self._n_neighbors,\n",
    "                    nn_metric,\n",
    "                    self._metric_kwds,\n",
    "                    self.angular_rp_forest,\n",
    "                    random_state,\n",
    "                    self.low_memory,\n",
    "                    use_pynndescent=True,\n",
    "                    n_jobs=self.n_jobs,\n",
    "                    verbose=self.verbose,\n",
    "                )\n",
    "            else:\n",
    "                self._knn_indices = self.knn_indices\n",
    "                self._knn_dists = self.knn_dists\n",
    "                self._knn_search_index = self.knn_search_index\n",
    "            # Disconnect any vertices farther apart than _disconnection_distance\n",
    "            disconnected_index = self._knn_dists >= self._disconnection_distance\n",
    "            self._knn_indices[disconnected_index] = -1\n",
    "            self._knn_dists[disconnected_index] = np.inf\n",
    "            edges_removed = disconnected_index.sum()\n",
    "\n",
    "            (\n",
    "                self.graph_,\n",
    "                self._sigmas,\n",
    "                self._rhos,\n",
    "                self.graph_dists_,\n",
    "            ) = fuzzy_simplicial_set(\n",
    "                X[index],\n",
    "                self.n_neighbors,\n",
    "                random_state,\n",
    "                nn_metric,\n",
    "                self._metric_kwds,\n",
    "                self._knn_indices,\n",
    "                self._knn_dists,\n",
    "                self.angular_rp_forest,\n",
    "                self.set_op_mix_ratio,\n",
    "                self.local_connectivity,\n",
    "                True,\n",
    "                self.verbose,\n",
    "                self.densmap or self.output_dens,\n",
    "            )\n",
    "            # Report the number of vertices with degree 0 in our umap.graph_\n",
    "            # This ensures that they were properly disconnected.\n",
    "            vertices_disconnected = np.sum(\n",
    "                np.array(self.graph_.sum(axis=1)).flatten() == 0\n",
    "            )\n",
    "            raise_disconnected_warning(\n",
    "                edges_removed,\n",
    "                vertices_disconnected,\n",
    "                self._disconnection_distance,\n",
    "                self._raw_data.shape[0],\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "\n",
    "        # Currently not checking if any duplicate points have differing labels\n",
    "        # Might be worth throwing a warning...\n",
    "        if y is not None:\n",
    "            len_X = len(X) if not self._sparse_data else X.shape[0]\n",
    "            if len_X != len(y):\n",
    "                raise ValueError(\n",
    "                    \"Length of x = {len_x}, length of y = {len_y}, while it must be equal.\".format(\n",
    "                        len_x=len_X, len_y=len(y)\n",
    "                    )\n",
    "                )\n",
    "            if self.target_metric == \"string\":\n",
    "                y_ = y[index]\n",
    "            else:\n",
    "                y_ = check_array(y, ensure_2d=False)[index]\n",
    "            if self.target_metric == \"categorical\":\n",
    "                if self.target_weight < 1.0:\n",
    "                    far_dist = 2.5 * (1.0 / (1.0 - self.target_weight))\n",
    "                else:\n",
    "                    far_dist = 1.0e12\n",
    "                self.graph_ = discrete_metric_simplicial_set_intersection(\n",
    "                    self.graph_, y_, far_dist=far_dist\n",
    "                )\n",
    "            elif self.target_metric in dist.DISCRETE_METRICS:\n",
    "                if self.target_weight < 1.0:\n",
    "                    scale = 2.5 * (1.0 / (1.0 - self.target_weight))\n",
    "                else:\n",
    "                    scale = 1.0e12\n",
    "                # self.graph_ = discrete_metric_simplicial_set_intersection(\n",
    "                #     self.graph_,\n",
    "                #     y_,\n",
    "                #     metric=self.target_metric,\n",
    "                #     metric_kws=self.target_metric_kwds,\n",
    "                #     metric_scale=scale\n",
    "                # )\n",
    "\n",
    "                metric_kws = dist.get_discrete_params(y_, self.target_metric)\n",
    "\n",
    "                self.graph_ = discrete_metric_simplicial_set_intersection(\n",
    "                    self.graph_,\n",
    "                    y_,\n",
    "                    metric=self.target_metric,\n",
    "                    metric_kws=metric_kws,\n",
    "                    metric_scale=scale,\n",
    "                )\n",
    "            else:\n",
    "                if len(y_.shape) == 1:\n",
    "                    y_ = y_.reshape(-1, 1)\n",
    "                if self.target_n_neighbors == -1:\n",
    "                    target_n_neighbors = self._n_neighbors\n",
    "                else:\n",
    "                    target_n_neighbors = self.target_n_neighbors\n",
    "\n",
    "                # Handle the small case as precomputed as before\n",
    "                if y.shape[0] < 4096:\n",
    "                    try:\n",
    "                        ydmat = pairwise_distances(\n",
    "                            y_, metric=self.target_metric, **self._target_metric_kwds\n",
    "                        )\n",
    "                    except (TypeError, ValueError):\n",
    "                        ydmat = dist.pairwise_special_metric(\n",
    "                            y_,\n",
    "                            metric=self.target_metric,\n",
    "                            kwds=self._target_metric_kwds,\n",
    "                        )\n",
    "\n",
    "                    (target_graph, target_sigmas, target_rhos,) = fuzzy_simplicial_set(\n",
    "                        ydmat,\n",
    "                        target_n_neighbors,\n",
    "                        random_state,\n",
    "                        \"precomputed\",\n",
    "                        self._target_metric_kwds,\n",
    "                        None,\n",
    "                        None,\n",
    "                        False,\n",
    "                        1.0,\n",
    "                        1.0,\n",
    "                        False,\n",
    "                    )\n",
    "                else:\n",
    "                    # Standard case\n",
    "                    (target_graph, target_sigmas, target_rhos,) = fuzzy_simplicial_set(\n",
    "                        y_,\n",
    "                        target_n_neighbors,\n",
    "                        random_state,\n",
    "                        self.target_metric,\n",
    "                        self._target_metric_kwds,\n",
    "                        None,\n",
    "                        None,\n",
    "                        False,\n",
    "                        1.0,\n",
    "                        1.0,\n",
    "                        False,\n",
    "                    )\n",
    "                # product = self.graph_.multiply(target_graph)\n",
    "                # # self.graph_ = 0.99 * product + 0.01 * (self.graph_ +\n",
    "                # #                                        target_graph -\n",
    "                # #                                        product)\n",
    "                # self.graph_ = product\n",
    "                self.graph_ = general_simplicial_set_intersection(\n",
    "                    self.graph_, target_graph, self.target_weight\n",
    "                )\n",
    "                self.graph_ = reset_local_connectivity(self.graph_)\n",
    "            self._supervised = True\n",
    "        else:\n",
    "            self._supervised = False\n",
    "\n",
    "        if self.densmap or self.output_dens:\n",
    "            self._densmap_kwds[\"graph_dists\"] = self.graph_dists_\n",
    "\n",
    "        if self.verbose:\n",
    "            print(ts(), \"Construct embedding\")\n",
    "\n",
    "        if self.transform_mode == \"embedding\":\n",
    "            epochs = self.n_epochs_list if self.n_epochs_list is not None else self.n_epochs\n",
    "            self.embedding_, aux_data = self._fit_embed_data(\n",
    "                self._raw_data[index],\n",
    "                epochs,\n",
    "                init,\n",
    "                random_state,  # JH why raw data?\n",
    "            )\n",
    "\n",
    "            if self.n_epochs_list is not None:\n",
    "                if \"embedding_list\" not in aux_data:\n",
    "                    raise KeyError(\"No list of embedding were found in 'aux_data'. \"\n",
    "                                   \"It is likely the layout optimization function \"\n",
    "                                   \"doesn't support the list of int for 'n_epochs'.\")\n",
    "                else:\n",
    "                    self.embedding_list_ = [e[inverse] for e in aux_data[\"embedding_list\"]]\n",
    "\n",
    "            # Assign any points that are fully disconnected from our manifold(s) to have embedding\n",
    "            # coordinates of np.nan.  These will be filtered by our plotting functions automatically.\n",
    "            # They also prevent users from being deceived a distance query to one of these points.\n",
    "            # Might be worth moving this into simplicial_set_embedding or _fit_embed_data\n",
    "            disconnected_vertices = np.array(self.graph_.sum(axis=1)).flatten() == 0\n",
    "            if len(disconnected_vertices) > 0:\n",
    "                self.embedding_[disconnected_vertices] = np.full(\n",
    "                    self.n_components, np.nan\n",
    "                )\n",
    "\n",
    "            self.embedding_ = self.embedding_[inverse]\n",
    "            if self.output_dens:\n",
    "                self.rad_orig_ = aux_data[\"rad_orig\"][inverse]\n",
    "                self.rad_emb_ = aux_data[\"rad_emb\"][inverse]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(ts() + \" Finished embedding\")\n",
    "\n",
    "        numba.set_num_threads(self._original_n_threads)\n",
    "        self._input_hash = joblib.hash(self._raw_data)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9f195-620f-4391-a56a-ada09a259fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fit_embed_data(self, X, n_epochs, init, random_state):\n",
    "        \"\"\"A method wrapper for simplicial_set_embedding that can be\n",
    "        replaced by subclasses.\n",
    "        \"\"\"\n",
    "        return simplicial_set_embedding(\n",
    "            X,\n",
    "            self.graph_,\n",
    "            self.n_components,\n",
    "            self._initial_alpha,\n",
    "            self._a,\n",
    "            self._b,\n",
    "            self.repulsion_strength,\n",
    "            self.negative_sample_rate,\n",
    "            n_epochs,\n",
    "            init,\n",
    "            random_state,\n",
    "            self._input_distance_func,\n",
    "            self._metric_kwds,\n",
    "            self.densmap,\n",
    "            self._densmap_kwds,\n",
    "            self.output_dens,\n",
    "            self._output_distance_func,\n",
    "            self._output_metric_kwds,\n",
    "            self.output_metric in (\"euclidean\", \"l2\"),\n",
    "            self.random_state is None,\n",
    "            self.verbose,\n",
    "            tqdm_kwds=self.tqdm_kwds,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cea18475-d033-452f-9e9f-1b90791759c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(self, X, y=None):\n",
    "    print(\"using correct function\n",
    "        \"\"\"Fit X into an embedded space and return that transformed\n",
    "        output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "            If the metric is 'precomputed' X must be a square distance\n",
    "            matrix. Otherwise it contains a sample per row.\n",
    "        y : array, shape (n_samples)\n",
    "            A target array for supervised dimension reduction. How this is\n",
    "            handled is determined by parameters UMAP was instantiated with.\n",
    "            The relevant attributes are ``target_metric`` and\n",
    "            ``target_metric_kwds``.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_components)\n",
    "            Embedding of the training data in low-dimensional space.\n",
    "        or a tuple (X_new, r_orig, r_emb) if ``output_dens`` flag is set,\n",
    "        which additionally includes:\n",
    "        r_orig: array, shape (n_samples)\n",
    "            Local radii of data points in the original data space (log-transformed).\n",
    "        r_emb: array, shape (n_samples)\n",
    "            Local radii of data points in the embedding (log-transformed).\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        if self.transform_mode == \"embedding\":\n",
    "            if self.output_dens:\n",
    "                return self.embedding_, self.rad_orig_, self.rad_emb_\n",
    "            else:\n",
    "                return self.embedding_\n",
    "        elif self.transform_mode == \"graph\":\n",
    "            return self.graph_\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Unrecognized transform mode {}; should be one of 'embedding' or 'graph'\".format(\n",
    "                    self.transform_mode\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9caa5b-97da-46f0-b898-c13be5a9cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(self, X):\n",
    "        \"\"\"Transform X into the existing embedded space and return that\n",
    "        transformed output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            New data to be transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_components)\n",
    "            Embedding of the new data in low-dimensional space.\n",
    "        \"\"\"\n",
    "        # If we fit just a single instance then error\n",
    "        if self._raw_data.shape[0] == 1:\n",
    "            raise ValueError(\n",
    "                \"Transform unavailable when model was fit with only a single data sample.\"\n",
    "            )\n",
    "        # If we just have the original input then short circuit things\n",
    "        X = check_array(X, dtype=np.float32, accept_sparse=\"csr\", order=\"C\")\n",
    "        x_hash = joblib.hash(X)\n",
    "        if x_hash == self._input_hash:\n",
    "            if self.transform_mode == \"embedding\":\n",
    "                return self.embedding_\n",
    "            elif self.transform_mode == \"graph\":\n",
    "                return self.graph_\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Unrecognized transform mode {}; should be one of 'embedding' or 'graph'\".format(\n",
    "                        self.transform_mode\n",
    "                    )\n",
    "                )\n",
    "        if self.densmap:\n",
    "            raise NotImplementedError(\n",
    "                \"Transforming data into an existing embedding not supported for densMAP.\"\n",
    "            )\n",
    "\n",
    "        # #848: knn_search_index is allowed to be None if not transforming new data,\n",
    "        # so now we must validate that if it exists it is not None\n",
    "        if hasattr(self, \"_knn_search_index\") and self._knn_search_index is None:\n",
    "            raise NotImplementedError(\n",
    "                \"No search index available: transforming data\"\n",
    "                \" into an existing embedding is not supported\"\n",
    "            )\n",
    "\n",
    "        # X = check_array(X, dtype=np.float32, order=\"C\", accept_sparse=\"csr\")\n",
    "        random_state = check_random_state(self.transform_seed)\n",
    "        rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n",
    "\n",
    "        if self.metric == \"precomputed\":\n",
    "            warn(\n",
    "                \"Transforming new data with precomputed metric. \"\n",
    "                \"We are assuming the input data is a matrix of distances from the new points \"\n",
    "                \"to the points in the training set. If the input matrix is sparse, it should \"\n",
    "                \"contain distances from the new points to their nearest neighbours \"\n",
    "                \"or approximate nearest neighbours in the training set.\"\n",
    "            )\n",
    "            assert X.shape[1] == self._raw_data.shape[0]\n",
    "            if scipy.sparse.issparse(X):\n",
    "                indices = np.full(\n",
    "                    (X.shape[0], self._n_neighbors), dtype=np.int32, fill_value=-1\n",
    "                )\n",
    "                dists = np.full_like(indices, dtype=np.float32, fill_value=-1)\n",
    "                for i in range(X.shape[0]):\n",
    "                    data_indices = np.argsort(X[i].data)\n",
    "                    if len(data_indices) < self._n_neighbors:\n",
    "                        raise ValueError(\n",
    "                            f\"Need at least n_neighbors ({self.n_neighbors}) distances for each row!\"\n",
    "                        )\n",
    "                    indices[i] = X[i].indices[data_indices[: self._n_neighbors]]\n",
    "                    dists[i] = X[i].data[data_indices[: self._n_neighbors]]\n",
    "            else:\n",
    "                indices = np.argsort(X, axis=1)[:, : self._n_neighbors].astype(np.int32)\n",
    "                dists = np.take_along_axis(X, indices, axis=1)\n",
    "            assert np.min(indices) >= 0 and np.min(dists) >= 0.0\n",
    "        elif self._small_data:\n",
    "            try:\n",
    "                # sklearn pairwise_distances fails for callable metric on sparse data\n",
    "                _m = self.metric if self._sparse_data else self._input_distance_func\n",
    "                dmat = pairwise_distances(\n",
    "                    X, self._raw_data, metric=_m, **self._metric_kwds\n",
    "                )\n",
    "            except (TypeError, ValueError):\n",
    "                # metric is numba.jit'd or not supported by sklearn,\n",
    "                # fallback to pairwise special\n",
    "                if self._sparse_data:\n",
    "                    # Get a fresh metric since we are casting to dense\n",
    "                    if not callable(self.metric):\n",
    "                        _m = dist.named_distances[self.metric]\n",
    "                        dmat = dist.pairwise_special_metric(\n",
    "                            X.toarray(),\n",
    "                            self._raw_data.toarray(),\n",
    "                            metric=_m,\n",
    "                            kwds=self._metric_kwds,\n",
    "                        )\n",
    "                    else:\n",
    "                        dmat = dist.pairwise_special_metric(\n",
    "                            X,\n",
    "                            self._raw_data,\n",
    "                            metric=self._input_distance_func,\n",
    "                            kwds=self._metric_kwds,\n",
    "                        )\n",
    "                else:\n",
    "                    dmat = dist.pairwise_special_metric(\n",
    "                        X,\n",
    "                        self._raw_data,\n",
    "                        metric=self._input_distance_func,\n",
    "                        kwds=self._metric_kwds,\n",
    "                    )\n",
    "            indices = np.argpartition(dmat, self._n_neighbors)[:, : self._n_neighbors]\n",
    "            dmat_shortened = submatrix(dmat, indices, self._n_neighbors)\n",
    "            indices_sorted = np.argsort(dmat_shortened)\n",
    "            indices = submatrix(indices, indices_sorted, self._n_neighbors)\n",
    "            dists = submatrix(dmat_shortened, indices_sorted, self._n_neighbors)\n",
    "        else:\n",
    "            epsilon = 0.24 if self._knn_search_index._angular_trees else 0.12\n",
    "            indices, dists = self._knn_search_index.query(\n",
    "                X, self.n_neighbors, epsilon=epsilon\n",
    "            )\n",
    "\n",
    "        dists = dists.astype(np.float32, order=\"C\")\n",
    "        # Remove any nearest neighbours who's distances are greater than our disconnection_distance\n",
    "        indices[dists >= self._disconnection_distance] = -1\n",
    "        adjusted_local_connectivity = max(0.0, self.local_connectivity - 1.0)\n",
    "        sigmas, rhos = smooth_knn_dist(\n",
    "            dists,\n",
    "            float(self._n_neighbors),\n",
    "            local_connectivity=float(adjusted_local_connectivity),\n",
    "        )\n",
    "\n",
    "        rows, cols, vals, dists = compute_membership_strengths(\n",
    "            indices, dists, sigmas, rhos, bipartite=True\n",
    "        )\n",
    "\n",
    "        graph = scipy.sparse.coo_matrix(\n",
    "            (vals, (rows, cols)), shape=(X.shape[0], self._raw_data.shape[0])\n",
    "        )\n",
    "\n",
    "        if self.transform_mode == \"graph\":\n",
    "            return graph\n",
    "\n",
    "        # This was a very specially constructed graph with constant degree.\n",
    "        # That lets us do fancy unpacking by reshaping the csr matrix indices\n",
    "        # and data. Doing so relies on the constant degree assumption!\n",
    "        # csr_graph = normalize(graph.tocsr(), norm=\"l1\")\n",
    "        # inds = csr_graph.indices.reshape(X.shape[0], self._n_neighbors)\n",
    "        # weights = csr_graph.data.reshape(X.shape[0], self._n_neighbors)\n",
    "        # embedding = init_transform(inds, weights, self.embedding_)\n",
    "        # This is less fast code than the above numba.jit'd code.\n",
    "        # It handles the fact that our nearest neighbour graph can now contain variable numbers of vertices.\n",
    "        csr_graph = graph.tocsr()\n",
    "        csr_graph.eliminate_zeros()\n",
    "        embedding = init_graph_transform(csr_graph, self.embedding_)\n",
    "\n",
    "        if self.n_epochs is None:\n",
    "            # For smaller datasets we can use more epochs\n",
    "            if graph.shape[0] <= 10000:\n",
    "                n_epochs = 100\n",
    "            else:\n",
    "                n_epochs = 30\n",
    "        else:\n",
    "            n_epochs = int(self.n_epochs // 3.0)\n",
    "\n",
    "        graph.data[graph.data < (graph.data.max() / float(n_epochs))] = 0.0\n",
    "        graph.eliminate_zeros()\n",
    "\n",
    "        epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n",
    "\n",
    "        head = graph.row\n",
    "        tail = graph.col\n",
    "        weight = graph.data\n",
    "\n",
    "        # optimize_layout = make_optimize_layout(\n",
    "        #     self._output_distance_func,\n",
    "        #     tuple(self.output_metric_kwds.values()),\n",
    "        # )\n",
    "\n",
    "        if self.output_metric == \"euclidean\":\n",
    "            embedding = optimize_layout_euclidean(\n",
    "                embedding,\n",
    "                self.embedding_.astype(np.float32, copy=True),  # Fixes #179 & #217,\n",
    "                head,\n",
    "                tail,\n",
    "                n_epochs,\n",
    "                graph.shape[1],\n",
    "                epochs_per_sample,\n",
    "                self._a,\n",
    "                self._b,\n",
    "                rng_state,\n",
    "                self.repulsion_strength,\n",
    "                self._initial_alpha / 4.0,\n",
    "                self.negative_sample_rate,\n",
    "                self.random_state is None,\n",
    "                verbose=self.verbose,\n",
    "                tqdm_kwds=self.tqdm_kwds,\n",
    "            )\n",
    "        else:\n",
    "            embedding = optimize_layout_generic(\n",
    "                embedding,\n",
    "                self.embedding_.astype(np.float32, copy=True),  # Fixes #179 & #217\n",
    "                head,\n",
    "                tail,\n",
    "                n_epochs,\n",
    "                graph.shape[1],\n",
    "                epochs_per_sample,\n",
    "                self._a,\n",
    "                self._b,\n",
    "                rng_state,\n",
    "                self.repulsion_strength,\n",
    "                self._initial_alpha / 4.0,\n",
    "                self.negative_sample_rate,\n",
    "                self._output_distance_func,\n",
    "                tuple(self._output_metric_kwds.values()),\n",
    "                verbose=self.verbose,\n",
    "                tqdm_kwds=self.tqdm_kwds,\n",
    "            )\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af58f998-2f29-43f8-897b-2a1ac1068653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(self, X):\n",
    "        \"\"\"Transform X in the existing embedded space back into the input\n",
    "        data space and return that transformed output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_components)\n",
    "            New points to be inverse transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_features)\n",
    "            Generated data points new data in data space.\n",
    "        \"\"\"\n",
    "\n",
    "        if self._sparse_data:\n",
    "            raise ValueError(\"Inverse transform not available for sparse input.\")\n",
    "        elif self._inverse_distance_func is None:\n",
    "            raise ValueError(\"Inverse transform not available for given metric.\")\n",
    "        elif self.densmap:\n",
    "            raise ValueError(\"Inverse transform not available for densMAP.\")\n",
    "        elif self.n_components >= 8:\n",
    "            warn(\n",
    "                \"Inverse transform works best with low dimensional embeddings.\"\n",
    "                \" Results may be poor, or this approach to inverse transform\"\n",
    "                \" may fail altogether! If you need a high dimensional latent\"\n",
    "                \" space and inverse transform operations consider using an\"\n",
    "                \" autoencoder.\"\n",
    "            )\n",
    "        elif self.transform_mode == \"graph\":\n",
    "            raise ValueError(\n",
    "                \"Inverse transform not available for transform_mode = 'graph'\"\n",
    "            )\n",
    "\n",
    "        X = check_array(X, dtype=np.float32, order=\"C\")\n",
    "        random_state = check_random_state(self.transform_seed)\n",
    "        rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n",
    "\n",
    "        # build Delaunay complex (Does this not assume a roughly euclidean output metric)?\n",
    "        deltri = scipy.spatial.Delaunay(\n",
    "            self.embedding_, incremental=True, qhull_options=\"QJ\"\n",
    "        )\n",
    "        neighbors = deltri.simplices[deltri.find_simplex(X)]\n",
    "        adjmat = scipy.sparse.lil_matrix(\n",
    "            (self.embedding_.shape[0], self.embedding_.shape[0]), dtype=int\n",
    "        )\n",
    "        for i in np.arange(0, deltri.simplices.shape[0]):\n",
    "            for j in deltri.simplices[i]:\n",
    "                if j < self.embedding_.shape[0]:\n",
    "                    idx = deltri.simplices[i][\n",
    "                        deltri.simplices[i] < self.embedding_.shape[0]\n",
    "                    ]\n",
    "                    adjmat[j, idx] = 1\n",
    "                    adjmat[idx, j] = 1\n",
    "\n",
    "        adjmat = scipy.sparse.csr_matrix(adjmat)\n",
    "\n",
    "        min_vertices = min(self._raw_data.shape[-1], self._raw_data.shape[0])\n",
    "\n",
    "        neighborhood = [\n",
    "            breadth_first_search(adjmat, v[0], min_vertices=min_vertices)\n",
    "            for v in neighbors\n",
    "        ]\n",
    "        if callable(self.output_metric):\n",
    "            # need to create another numba.jit-able wrapper for callable\n",
    "            # output_metrics that return a tuple (already checked that it does\n",
    "            # during param validation in `fit` method)\n",
    "            _out_m = self.output_metric\n",
    "\n",
    "            @numba.njit(fastmath=True)\n",
    "            def _output_dist_only(x, y, *kwds):\n",
    "                return _out_m(x, y, *kwds)[0]\n",
    "\n",
    "            dist_only_func = _output_dist_only\n",
    "        elif self.output_metric in dist.named_distances.keys():\n",
    "            dist_only_func = dist.named_distances[self.output_metric]\n",
    "        else:\n",
    "            # shouldn't really ever get here because of checks already performed,\n",
    "            # but works as a failsafe in case attr was altered manually after fitting\n",
    "            raise ValueError(\n",
    "                \"Unrecognized output metric: {}\".format(self.output_metric)\n",
    "            )\n",
    "\n",
    "        dist_args = tuple(self._output_metric_kwds.values())\n",
    "        distances = [\n",
    "            np.array(\n",
    "                [\n",
    "                    dist_only_func(X[i], self.embedding_[nb], *dist_args)\n",
    "                    for nb in neighborhood[i]\n",
    "                ]\n",
    "            )\n",
    "            for i in range(X.shape[0])\n",
    "        ]\n",
    "        idx = np.array([np.argsort(e)[:min_vertices] for e in distances])\n",
    "\n",
    "        dists_output_space = np.array(\n",
    "            [distances[i][idx[i]] for i in range(len(distances))]\n",
    "        )\n",
    "        indices = np.array([neighborhood[i][idx[i]] for i in range(len(neighborhood))])\n",
    "\n",
    "        rows, cols, distances = np.array(\n",
    "            [\n",
    "                [i, indices[i, j], dists_output_space[i, j]]\n",
    "                for i in range(indices.shape[0])\n",
    "                for j in range(min_vertices)\n",
    "            ]\n",
    "        ).T\n",
    "\n",
    "        # calculate membership strength of each edge\n",
    "        weights = 1 / (1 + self._a * distances ** (2 * self._b))\n",
    "\n",
    "        # compute 1-skeleton\n",
    "        # convert 1-skeleton into coo_matrix adjacency matrix\n",
    "        graph = scipy.sparse.coo_matrix(\n",
    "            (weights, (rows, cols)), shape=(X.shape[0], self._raw_data.shape[0])\n",
    "        )\n",
    "\n",
    "        # That lets us do fancy unpacking by reshaping the csr matrix indices\n",
    "        # and data. Doing so relies on the constant degree assumption!\n",
    "        # csr_graph = graph.tocsr()\n",
    "        csr_graph = normalize(graph.tocsr(), norm=\"l1\")\n",
    "        inds = csr_graph.indices.reshape(X.shape[0], min_vertices)\n",
    "        weights = csr_graph.data.reshape(X.shape[0], min_vertices)\n",
    "        inv_transformed_points = init_transform(inds, weights, self._raw_data)\n",
    "\n",
    "        if self.n_epochs is None:\n",
    "            # For smaller datasets we can use more epochs\n",
    "            if graph.shape[0] <= 10000:\n",
    "                n_epochs = 100\n",
    "            else:\n",
    "                n_epochs = 30\n",
    "        else:\n",
    "            n_epochs = int(self.n_epochs // 3.0)\n",
    "\n",
    "        # graph.data[graph.data < (graph.data.max() / float(n_epochs))] = 0.0\n",
    "        # graph.eliminate_zeros()\n",
    "\n",
    "        epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n",
    "\n",
    "        head = graph.row\n",
    "        tail = graph.col\n",
    "        weight = graph.data\n",
    "\n",
    "        inv_transformed_points = optimize_layout_inverse(\n",
    "            inv_transformed_points,\n",
    "            self._raw_data,\n",
    "            head,\n",
    "            tail,\n",
    "            weight,\n",
    "            self._sigmas,\n",
    "            self._rhos,\n",
    "            n_epochs,\n",
    "            graph.shape[1],\n",
    "            epochs_per_sample,\n",
    "            self._a,\n",
    "            self._b,\n",
    "            rng_state,\n",
    "            self.repulsion_strength,\n",
    "            self._initial_alpha / 4.0,\n",
    "            self.negative_sample_rate,\n",
    "            self._inverse_distance_func,\n",
    "            tuple(self._metric_kwds.values()),\n",
    "            verbose=self.verbose,\n",
    "            tqdm_kwds=self.tqdm_kwds,\n",
    "        )\n",
    "\n",
    "        return inv_transformed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "885103ba-e0b0-4837-a0ef-b2fe1d62a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    " def update(self, X):\n",
    "        X = check_array(X, dtype=np.float32, accept_sparse=\"csr\", order=\"C\")\n",
    "        random_state = check_random_state(self.transform_seed)\n",
    "        rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n",
    "\n",
    "        original_size = self._raw_data.shape[0]\n",
    "\n",
    "        if self.metric == \"precomputed\":\n",
    "            raise ValueError(\"Update does not currently support precomputed metrics\")\n",
    "        if self._supervised:\n",
    "            raise ValueError(\"Updating supervised models is not currently \" \"supported\")\n",
    "\n",
    "        if self._small_data:\n",
    "\n",
    "            if self._sparse_data:\n",
    "                self._raw_data = scipy.sparse.vstack([self._raw_data, X])\n",
    "            else:\n",
    "                self._raw_data = np.vstack([self._raw_data, X])\n",
    "\n",
    "            if self._raw_data.shape[0] < 4096:\n",
    "                # still small data\n",
    "                try:\n",
    "                    # sklearn pairwise_distances fails for callable metric on sparse data\n",
    "                    _m = self.metric if self._sparse_data else self._input_distance_func\n",
    "                    dmat = pairwise_distances(\n",
    "                        self._raw_data, metric=_m, **self._metric_kwds\n",
    "                    )\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    # metric is numba.jit'd or not supported by sklearn,\n",
    "                    # fallback to pairwise special\n",
    "\n",
    "                    if self._sparse_data:\n",
    "                        # Get a fresh metric since we are casting to dense\n",
    "                        if not callable(self.metric):\n",
    "                            _m = dist.named_distances[self.metric]\n",
    "                            dmat = dist.pairwise_special_metric(\n",
    "                                self._raw_data.toarray(),\n",
    "                                metric=_m,\n",
    "                                kwds=self._metric_kwds,\n",
    "                            )\n",
    "                        else:\n",
    "                            dmat = dist.pairwise_special_metric(\n",
    "                                self._raw_data,\n",
    "                                metric=self._input_distance_func,\n",
    "                                kwds=self._metric_kwds,\n",
    "                            )\n",
    "                    else:\n",
    "                        dmat = dist.pairwise_special_metric(\n",
    "                            self._raw_data,\n",
    "                            metric=self._input_distance_func,\n",
    "                            kwds=self._metric_kwds,\n",
    "                        )\n",
    "                self.graph_, self._sigmas, self._rhos = fuzzy_simplicial_set(\n",
    "                    dmat,\n",
    "                    self._n_neighbors,\n",
    "                    random_state,\n",
    "                    \"precomputed\",\n",
    "                    self._metric_kwds,\n",
    "                    None,\n",
    "                    None,\n",
    "                    self.angular_rp_forest,\n",
    "                    self.set_op_mix_ratio,\n",
    "                    self.local_connectivity,\n",
    "                    True,\n",
    "                    self.verbose,\n",
    "                )\n",
    "                knn_indices = np.argsort(dmat)[:, : self.n_neighbors]\n",
    "            else:\n",
    "                # now large data\n",
    "                self._small_data = False\n",
    "                if self._sparse_data and self.metric in pynn_sparse_named_distances:\n",
    "                    nn_metric = self.metric\n",
    "                elif not self._sparse_data and self.metric in pynn_named_distances:\n",
    "                    nn_metric = self.metric\n",
    "                else:\n",
    "                    nn_metric = self._input_distance_func\n",
    "\n",
    "                (\n",
    "                    self._knn_indices,\n",
    "                    self._knn_dists,\n",
    "                    self._knn_search_index,\n",
    "                ) = nearest_neighbors(\n",
    "                    self._raw_data,\n",
    "                    self._n_neighbors,\n",
    "                    nn_metric,\n",
    "                    self._metric_kwds,\n",
    "                    self.angular_rp_forest,\n",
    "                    random_state,\n",
    "                    self.low_memory,\n",
    "                    use_pynndescent=True,\n",
    "                    n_jobs=self.n_jobs,\n",
    "                    verbose=self.verbose,\n",
    "                )\n",
    "\n",
    "                self.graph_, self._sigmas, self._rhos = fuzzy_simplicial_set(\n",
    "                    self._raw_data,\n",
    "                    self.n_neighbors,\n",
    "                    random_state,\n",
    "                    nn_metric,\n",
    "                    self._metric_kwds,\n",
    "                    self._knn_indices,\n",
    "                    self._knn_dists,\n",
    "                    self.angular_rp_forest,\n",
    "                    self.set_op_mix_ratio,\n",
    "                    self.local_connectivity,\n",
    "                    True,\n",
    "                    self.verbose,\n",
    "                )\n",
    "                knn_indices = self._knn_indices\n",
    "\n",
    "            init = np.zeros(\n",
    "                (self._raw_data.shape[0], self.n_components), dtype=np.float32\n",
    "            )\n",
    "            init[:original_size] = self.embedding_\n",
    "\n",
    "            init_update(init, original_size, knn_indices)\n",
    "            if self.n_epochs is None:\n",
    "                n_epochs = 0\n",
    "            else:\n",
    "                n_epochs = self.n_epochs\n",
    "\n",
    "            self.embedding_, aux_data = simplicial_set_embedding(\n",
    "                self._raw_data,\n",
    "                self.graph_,\n",
    "                self.n_components,\n",
    "                self._initial_alpha,\n",
    "                self._a,\n",
    "                self._b,\n",
    "                self.repulsion_strength,\n",
    "                self.negative_sample_rate,\n",
    "                n_epochs,\n",
    "                init,\n",
    "                random_state,\n",
    "                self._input_distance_func,\n",
    "                self._metric_kwds,\n",
    "                self.densmap,\n",
    "                self._densmap_kwds,\n",
    "                self.output_dens,\n",
    "                self._output_distance_func,\n",
    "                self._output_metric_kwds,\n",
    "                self.output_metric in (\"euclidean\", \"l2\"),\n",
    "                self.random_state is None,\n",
    "                self.verbose,\n",
    "                tqdm_kwds=self.tqdm_kwds,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self._knn_search_index.prepare()\n",
    "            self._knn_search_index.update(X)\n",
    "            self._raw_data = self._knn_search_index._raw_data\n",
    "            (\n",
    "                self._knn_indices,\n",
    "                self._knn_dists,\n",
    "            ) = self._knn_search_index.neighbor_graph\n",
    "\n",
    "            if self._sparse_data and self.metric in pynn_sparse_named_distances:\n",
    "                nn_metric = self.metric\n",
    "            elif not self._sparse_data and self.metric in pynn_named_distances:\n",
    "                nn_metric = self.metric\n",
    "            else:\n",
    "                nn_metric = self._input_distance_func\n",
    "\n",
    "            self.graph_, self._sigmas, self._rhos = fuzzy_simplicial_set(\n",
    "                self._raw_data,\n",
    "                self.n_neighbors,\n",
    "                random_state,\n",
    "                nn_metric,\n",
    "                self._metric_kwds,\n",
    "                self._knn_indices,\n",
    "                self._knn_dists,\n",
    "                self.angular_rp_forest,\n",
    "                self.set_op_mix_ratio,\n",
    "                self.local_connectivity,\n",
    "                True,\n",
    "                self.verbose,\n",
    "            )\n",
    "\n",
    "            init = np.zeros(\n",
    "                (self._raw_data.shape[0], self.n_components), dtype=np.float32\n",
    "            )\n",
    "            init[:original_size] = self.embedding_\n",
    "            init_update(init, original_size, self._knn_indices)\n",
    "\n",
    "            if self.n_epochs is None:\n",
    "                n_epochs = 0\n",
    "            else:\n",
    "                n_epochs = self.n_epochs\n",
    "\n",
    "            self.embedding_, aux_data = simplicial_set_embedding(\n",
    "                self._raw_data,\n",
    "                self.graph_,\n",
    "                self.n_components,\n",
    "                self._initial_alpha,\n",
    "                self._a,\n",
    "                self._b,\n",
    "                self.repulsion_strength,\n",
    "                self.negative_sample_rate,\n",
    "                n_epochs,\n",
    "                init,\n",
    "                random_state,\n",
    "                self._input_distance_func,\n",
    "                self._metric_kwds,\n",
    "                self.densmap,\n",
    "                self._densmap_kwds,\n",
    "                self.output_dens,\n",
    "                self._output_distance_func,\n",
    "                self._output_metric_kwds,\n",
    "                self.output_metric in (\"euclidean\", \"l2\"),\n",
    "                self.random_state is None,\n",
    "                self.verbose,\n",
    "                tqdm_kwds=self.tqdm_kwds,\n",
    "            )\n",
    "\n",
    "        if self.output_dens:\n",
    "            self.rad_orig_ = aux_data[\"rad_orig\"]\n",
    "            self.rad_emb_ = aux_data[\"rad_emb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "060b1a08-3f78-46fc-9383-718ae08d3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names_out(self, feature_names_out=None):\n",
    "        \"\"\"\n",
    "        Defines descriptive names for each output of the (fitted) estimator.\n",
    "        :param feature_names_out: Optional passthrough for feature names.\n",
    "        By default, feature names will be generated automatically.\n",
    "        :return: List of descriptive names for each output variable from the fitted estimator.\n",
    "        \"\"\"\n",
    "        if feature_names_out is None:\n",
    "            feature_names_out = [f\"umap_component_{i+1}\" for i in range(self.n_components)]\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0076ea07-3d12-46c1-a1d0-389eb9f640d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __repr__(self):\n",
    "        from sklearn.utils._pprint import _EstimatorPrettyPrinter\n",
    "        import re\n",
    "\n",
    "        pp = _EstimatorPrettyPrinter(\n",
    "            compact=True,\n",
    "            indent=1,\n",
    "            indent_at_name=True,\n",
    "            n_max_elements_to_show=50,\n",
    "        )\n",
    "        pp._changed_only = True\n",
    "        repr_ = pp.pformat(self)\n",
    "        repr_ = re.sub(\"tqdm_kwds={.*},\", \"\", repr_, flags=re.S)\n",
    "        # remove empty lines\n",
    "        repr_ = re.sub(\"\\n *\\n\", \"\\n\", repr_, flags=re.S)\n",
    "        # remove extra whitespaces after a comma\n",
    "        repr_ = re.sub(\", +\", \", \", repr_)\n",
    "        return repr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7c5f0922-6327-4408-8166-7f651b84ae84",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UMAP' object has no attribute 'fit_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UMAP' object has no attribute 'fit_transform'"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "data = np.random.rand(800, 4)\n",
    "fit = UMAP()\n",
    "%time u = fit.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "557b8868-8b73-40e1-86d1-783ca6a4519f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mu\u001b[49m[:,\u001b[38;5;241m0\u001b[39m], u[:,\u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUMAP embedding of random colours\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(u[:,0], u[:,1], c=data)\n",
    "plt.title('UMAP embedding of random colours');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99dd6e-135f-4eee-b4e3-cc934afbadbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea89f3f-89a2-485a-bcc0-ab24fc23b351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e66dc-39ba-4e14-a499-f16f0a89ad57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe193e-2d3c-4375-9b1f-6e8408c47c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf595e9-dca7-40c6-9ebb-ece6be410af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
