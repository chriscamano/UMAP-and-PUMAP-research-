{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b867291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93412a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class svgd_bayesnn:\n",
    "\n",
    "    '''\n",
    "        We define a one-hidden-layer-neural-network specifically. We leave extension of deep neural network as our future work.\n",
    "        \n",
    "        Input\n",
    "            -- X_train: training dataset, features\n",
    "            -- y_train: training labels\n",
    "            -- batch_size: sub-sampling batch size\n",
    "            -- max_iter: maximum iterations for the training procedure\n",
    "            -- M: number of particles are used to fit the posterior distribution\n",
    "            -- n_hidden: number of hidden units\n",
    "            -- a0, b0: hyper-parameters of Gamma distribution\n",
    "            -- master_stepsize, auto_corr: parameters of adgrad\n",
    "    '''\n",
    "    def __init__(self, X_train, y_train,  batch_size = 100, max_iter = 1000, M = 20, n_hidden = 50, a0 = 1, b0 = 0.1, master_stepsize = 1e-3, auto_corr = 0.9):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.d = X_train.shape[1]   # number of data, dimension \n",
    "        self.M = M\n",
    "        \n",
    "        num_vars = self.d * n_hidden + n_hidden * 2 + 3  # w1: d*n_hidden; b1: n_hidden; w2 = n_hidden; b2 = 1; 2 variances\n",
    "        self.theta = np.zeros([self.M, num_vars])  # particles, will be initialized later\n",
    "        \n",
    "        '''\n",
    "            We keep the last 10% (maximum 500) of training data points for model developing\n",
    "        '''\n",
    "        size_dev = min(int(np.round(0.1 * X_train.shape[0])), 500)\n",
    "        X_dev, y_dev = X_train[-size_dev:], y_train[-size_dev:]\n",
    "        X_train, y_train = X_train[:-size_dev], y_train[:-size_dev]\n",
    "\n",
    "        '''\n",
    "            The data sets are normalized so that the input features and the targets have zero mean and unit variance\n",
    "        '''\n",
    "        self.std_X_train = np.std(X_train, 0)\n",
    "        self.std_X_train[ self.std_X_train == 0 ] = 1\n",
    "        self.mean_X_train = np.mean(X_train, 0)\n",
    "                \n",
    "        self.mean_y_train = np.mean(y_train)\n",
    "        self.std_y_train = np.std(y_train)\n",
    "        \n",
    "        '''\n",
    "            Theano symbolic variables\n",
    "            Define the neural network here\n",
    "        '''\n",
    "        X = T.matrix('X') # Feature matrix\n",
    "        y = T.vector('y') # labels\n",
    "        \n",
    "        w_1 = T.matrix('w_1') # weights between input layer and hidden layer\n",
    "        b_1 = T.vector('b_1') # bias vector of hidden layer\n",
    "        w_2 = T.vector('w_2') # weights between hidden layer and output layer\n",
    "        b_2 = T.scalar('b_2') # bias of output\n",
    "        \n",
    "        N = T.scalar('N') # number of observations\n",
    "        \n",
    "        log_gamma = T.scalar('log_gamma')   # variances related parameters\n",
    "        log_lambda = T.scalar('log_lambda')\n",
    "        \n",
    "        ###\n",
    "        prediction = T.dot(T.nnet.relu(T.dot(X, w_1)+b_1), w_2) + b_2\n",
    "        \n",
    "        ''' define the log posterior distribution '''\n",
    "        log_lik_data = -0.5 * X.shape[0] * (T.log(2*np.pi) - log_gamma) - (T.exp(log_gamma)/2) * T.sum(T.power(prediction - y, 2))\n",
    "        log_prior_data = (a0 - 1) * log_gamma - b0 * T.exp(log_gamma) + log_gamma\n",
    "        log_prior_w = -0.5 * (num_vars-2) * (T.log(2*np.pi)-log_lambda) - (T.exp(log_lambda)/2)*((w_1**2).sum() + (w_2**2).sum() + (b_1**2).sum() + b_2**2)  \\\n",
    "                       + (a0-1) * log_lambda - b0 * T.exp(log_lambda) + log_lambda\n",
    "        \n",
    "        # sub-sampling mini-batches of data, where (X, y) is the batch data, and N is the number of whole observations\n",
    "        log_posterior = (log_lik_data * N / X.shape[0] + log_prior_data + log_prior_w)\n",
    "        dw_1, db_1, dw_2, db_2, d_log_gamma, d_log_lambda = T.grad(log_posterior, [w_1, b_1, w_2, b_2, log_gamma, log_lambda])\n",
    "        \n",
    "        # automatic gradient\n",
    "        logp_gradient = theano.function(\n",
    "             inputs = [X, y, w_1, b_1, w_2, b_2, log_gamma, log_lambda, N],\n",
    "             outputs = [dw_1, db_1, dw_2, db_2, d_log_gamma, d_log_lambda]\n",
    "        )\n",
    "        \n",
    "        # prediction function\n",
    "        self.nn_predict = theano.function(inputs = [X, w_1, b_1, w_2, b_2], outputs = prediction)\n",
    "        \n",
    "        '''\n",
    "            Training with SVGD\n",
    "        '''\n",
    "        # normalization\n",
    "        X_train, y_train = self.normalization(X_train, y_train)\n",
    "        N0 = X_train.shape[0]  # number of observations\n",
    "        \n",
    "        ''' initializing all particles '''\n",
    "        for i in range(self.M):\n",
    "            w1, b1, w2, b2, loggamma, loglambda = self.init_weights(a0, b0)\n",
    "            # use better initialization for gamma\n",
    "            ridx = np.random.choice(range(X_train.shape[0]), \\\n",
    "                                           np.min([X_train.shape[0], 1000]), replace = False)\n",
    "            y_hat = self.nn_predict(X_train[ridx,:], w1, b1, w2, b2)\n",
    "            loggamma = -np.log(np.mean(np.power(y_hat - y_train[ridx], 2)))\n",
    "            self.theta[i,:] = self.pack_weights(w1, b1, w2, b2, loggamma, loglambda)\n",
    "\n",
    "        grad_theta = np.zeros([self.M, num_vars])  # gradient \n",
    "        # adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "        for iter in range(max_iter):\n",
    "            # sub-sampling\n",
    "            batch = [ i % N0 for i in range(iter * batch_size, (iter + 1) * batch_size) ]\n",
    "            for i in range(self.M):\n",
    "                w1, b1, w2, b2, loggamma, loglambda = self.unpack_weights(self.theta[i,:])\n",
    "                dw1, db1, dw2, db2, dloggamma, dloglambda = logp_gradient(X_train[batch,:], y_train[batch], w1, b1, w2, b2, loggamma, loglambda, N0)\n",
    "                grad_theta[i,:] = self.pack_weights(dw1, db1, dw2, db2, dloggamma, dloglambda)\n",
    "                \n",
    "            # calculating the kernel matrix\n",
    "            kxy, dxkxy = self.svgd_kernel(h=-1)  \n",
    "            grad_theta = (np.matmul(kxy, grad_theta) + dxkxy) / self.M   # \\Phi(x)\n",
    "            \n",
    "            # adagrad \n",
    "            if iter == 0:\n",
    "                historical_grad = historical_grad + np.multiply(grad_theta, grad_theta)\n",
    "            else:\n",
    "                historical_grad = auto_corr * historical_grad + (1 - auto_corr) * np.multiply(grad_theta, grad_theta)\n",
    "            adj_grad = np.divide(grad_theta, fudge_factor+np.sqrt(historical_grad))\n",
    "            self.theta = self.theta + master_stepsize * adj_grad \n",
    "\n",
    "        '''\n",
    "            Model selection by using a development set\n",
    "        '''\n",
    "        X_dev = self.normalization(X_dev) \n",
    "        for i in range(self.M):\n",
    "            w1, b1, w2, b2, loggamma, loglambda = self.unpack_weights(self.theta[i, :])\n",
    "            pred_y_dev = self.nn_predict(X_dev, w1, b1, w2, b2) * self.std_y_train + self.mean_y_train\n",
    "            # likelihood\n",
    "            def f_log_lik(loggamma): return np.sum(  np.log(np.sqrt(np.exp(loggamma)) /np.sqrt(2*np.pi) * np.exp( -1 * (np.power(pred_y_dev - y_dev, 2) / 2) * np.exp(loggamma) )) )\n",
    "            # The higher probability is better    \n",
    "            lik1 = f_log_lik(loggamma)\n",
    "            # one heuristic setting\n",
    "            loggamma = -np.log(np.mean(np.power(pred_y_dev - y_dev, 2)))\n",
    "            lik2 = f_log_lik(loggamma)\n",
    "            if lik2 > lik1:\n",
    "                self.theta[i,-2] = loggamma  # update loggamma\n",
    "\n",
    "\n",
    "    def normalization(self, X, y = None):\n",
    "        X = (X - np.full(X.shape, self.mean_X_train)) / \\\n",
    "            np.full(X.shape, self.std_X_train)\n",
    "            \n",
    "        if y is not None:\n",
    "            y = (y - self.mean_y_train) / self.std_y_train\n",
    "            return (X, y)  \n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    '''\n",
    "        Initialize all particles\n",
    "    '''\n",
    "    def init_weights(self, a0, b0):\n",
    "        w1 = 1.0 / np.sqrt(self.d + 1) * np.random.randn(self.d, self.n_hidden)\n",
    "        b1 = np.zeros((self.n_hidden,))\n",
    "        w2 = 1.0 / np.sqrt(self.n_hidden + 1) * np.random.randn(self.n_hidden)\n",
    "        b2 = 0.\n",
    "        loggamma = np.log(np.random.gamma(a0, b0))\n",
    "        loglambda = np.log(np.random.gamma(a0, b0))\n",
    "        return (w1, b1, w2, b2, loggamma, loglambda)\n",
    "    \n",
    "    '''\n",
    "        Calculate kernel matrix and its gradient: K, \\nabla_x k\n",
    "    ''' \n",
    "    def svgd_kernel(self, h = -1):\n",
    "        sq_dist = pdist(self.theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = np.sqrt(0.5 * h / np.log(self.theta.shape[0]+1))\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        \n",
    "        Kxy = np.exp( -pairwise_dists / h**2 / 2)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, self.theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(self.theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(self.theta[:,i],sumkxy)\n",
    "        dxkxy = dxkxy / (h**2)\n",
    "        return (Kxy, dxkxy)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        Pack all parameters in our model\n",
    "    '''    \n",
    "    def pack_weights(self, w1, b1, w2, b2, loggamma, loglambda):\n",
    "        params = np.concatenate([w1.flatten(), b1, w2, [b2], [loggamma],[loglambda]])\n",
    "        return params\n",
    "    \n",
    "    '''\n",
    "        Unpack all parameters in our model\n",
    "    '''\n",
    "    def unpack_weights(self, z):\n",
    "        w = z\n",
    "        w1 = np.reshape(w[:self.d*self.n_hidden], [self.d, self.n_hidden])\n",
    "        b1 = w[self.d*self.n_hidden:(self.d+1)*self.n_hidden]\n",
    "    \n",
    "        w = w[(self.d+1)*self.n_hidden:]\n",
    "        w2, b2 = w[:self.n_hidden], w[-3] \n",
    "        \n",
    "        # the last two parameters are log variance\n",
    "        loggamma, loglambda= w[-2], w[-1]\n",
    "        \n",
    "        return (w1, b1, w2, b2, loggamma, loglambda)\n",
    "\n",
    "    \n",
    "    '''\n",
    "        Evaluating testing rmse and log-likelihood, which is the same as in PBP \n",
    "        Input:\n",
    "            -- X_test: unnormalized testing feature set\n",
    "            -- y_test: unnormalized testing labels\n",
    "    '''\n",
    "    def evaluation(self, X_test, y_test):\n",
    "        # normalization\n",
    "        X_test = self.normalization(X_test)\n",
    "        \n",
    "        # average over the output\n",
    "        pred_y_test = np.zeros([self.M, len(y_test)])\n",
    "        prob = np.zeros([self.M, len(y_test)])\n",
    "        \n",
    "        '''\n",
    "            Since we have M particles, we use a Bayesian view to calculate rmse and log-likelihood\n",
    "        '''\n",
    "        for i in range(self.M):\n",
    "            w1, b1, w2, b2, loggamma, loglambda = self.unpack_weights(self.theta[i, :])\n",
    "            pred_y_test[i, :] = self.nn_predict(X_test, w1, b1, w2, b2) * self.std_y_train + self.mean_y_train\n",
    "            prob[i, :] = np.sqrt(np.exp(loggamma)) /np.sqrt(2*np.pi) * np.exp( -1 * (np.power(pred_y_test[i, :] - y_test, 2) / 2) * np.exp(loggamma) )\n",
    "        pred = np.mean(pred_y_test, axis=0)\n",
    "        \n",
    "        # evaluation\n",
    "        svgd_rmse = np.sqrt(np.mean((pred - y_test)**2))\n",
    "        svgd_ll = np.mean(np.log(np.mean(prob, axis = 0)))\n",
    "        \n",
    "        return (svgd_rmse, svgd_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1519d567",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1245557996.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    np.random.seed(1)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "print( 'Theano', theano.version.version  )  #our implementation is based on theano 0.8.2\n",
    "               \n",
    "np.random.seed(1)\n",
    "''' load data file '''\n",
    "data = np.loadtxt('../data/boston_housing')\n",
    "\n",
    "# Please make sure that the last column is the label and the other columns are features\n",
    "X_input = data[ :, range(data.shape[ 1 ] - 1) ]\n",
    "y_input = data[ :, data.shape[ 1 ] - 1 ]\n",
    "\n",
    "''' build the training and testing data set'''\n",
    "train_ratio = 0.9 # We create the train and test sets with 90% and 10% of the data\n",
    "permutation = np.arange(X_input.shape[0])\n",
    "random.shuffle(permutation) \n",
    "\n",
    "size_train = int(np.round(X_input.shape[ 0 ] * train_ratio))\n",
    "index_train = permutation[ 0 : size_train]\n",
    "index_test = permutation[ size_train : ]\n",
    "\n",
    "X_train, y_train = X_input[ index_train, : ], y_input[ index_train ]\n",
    "X_test, y_test = X_input[ index_test, : ], y_input[ index_test ]\n",
    "\n",
    "start = time.time()\n",
    "''' Training Bayesian neural network with SVGD '''\n",
    "batch_size, n_hidden, max_iter = 100, 50, 2000  # max_iter is a trade-off between running time and performance\n",
    "svgd = svgd_bayesnn(X_train, y_train, batch_size = batch_size, n_hidden = n_hidden, max_iter = max_iter)\n",
    "svgd_time = time.time() - start\n",
    "svgd_rmse, svgd_ll = svgd.evaluation(X_test, y_test)\n",
    "print ('SVGD', svgd_rmse, svgd_ll, svgd_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afe80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
